{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nrh146/Documents/cc-genealogy/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from torchdata.stateful_dataloader import StatefulDataLoader\n",
    "from new_pipeline import process_data\n",
    "from huggingface_hub import HfApi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"allenai/c4\", \"en\", split=\"train\", streaming=True)\n",
    "ds = ds.remove_columns([\"text\", \"timestamp\"])\n",
    "ds = ds.map(process_data)\n",
    "ds = ds.with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = StatefulDataLoader(ds, batch_size=100, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1:\n",
      "Number of examples in batch: 100\n",
      "Sample url: https://klyq.com/beginners-bbq-class-taking-place-in-missoula/...\n",
      "Batch 2:\n",
      "Number of examples in batch: 100\n",
      "Sample url: http://sevenavedesign.com/studio/...\n",
      "Batch 3:\n",
      "Number of examples in batch: 100\n",
      "Sample url: http://help.browntape.com/support/solutions/articles/1000176729-adding-myntra-as-a-channel...\n",
      "Batch 4:\n",
      "Number of examples in batch: 100\n",
      "Sample url: http://www.excellence-in-stress-management.eu/surveys/...\n",
      "Batch 5:\n",
      "Number of examples in batch: 100\n",
      "Sample url: https://cdpug.org/free-breakfast-lecture-series-for-the-creative-community/...\n"
     ]
    }
   ],
   "source": [
    "# Iterate through the streaming dataset using the dataloader\n",
    "# We'll get batches of 100 examples each\n",
    "for i, batch in enumerate(dataloader):\n",
    "    # Print a sample of the batch\n",
    "    print(f\"Batch {i+1}:\")\n",
    "    print(f\"Number of examples in batch: {len(batch['url'])}\")\n",
    "    print(f\"Sample url: {batch['url'][0][:100]}...\")  # Show first 100 chars of first example\n",
    "    \n",
    "    # Optional: process or save your batch here\n",
    "    \n",
    "    # Break after a few batches to avoid endless iteration\n",
    "    if i >= 4:  # Show 5 batches\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"allenai/c4\", \"en\", split=\"train\", streaming=True)\n",
    "ds = ds.remove_columns([\"text\", \"timestamp\"])\n",
    "ds = ds.map(process_data, batched=True, batch_size=1_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RepoUrl('https://huggingface.co/datasets/nhagar/test-upload-c4', endpoint='https://huggingface.co', repo_type='dataset', repo_id='nhagar/test-upload-c4')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset(\"allenai/c4\", \"en\", split=\"train\", streaming=True)\n",
    "ds = ds.remove_columns([\"text\", \"timestamp\"])\n",
    "ds = ds.map(process_data, batched=True, batch_size=1_000)\n",
    "\n",
    "api = HfApi()\n",
    "repo_id = \"nhagar/test-upload-c4\"\n",
    "api.create_repo(repo_id, exist_ok=True, repo_type=\"dataset\")\n",
    "\n",
    "chunk_size = 100_000\n",
    "for i, chunk_ds in enumerate(ds.iter(batch_size=chunk_size)):\n",
    "    chunk_regular = Dataset.from_dict(chunk_ds)\n",
    "\n",
    "    chunk_regular.push_to_hub(\n",
    "        repo_id=repo_id,\n",
    "        data_dir=f\"chunks/chunk_{i}\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 200/200 [00:00<00:00, 2902.05ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.18it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 200/200 [00:00<00:00, 3083.05ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.05it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 200/200 [00:00<00:00, 2601.36ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.31it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 200/200 [00:00<00:00, 3094.67ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.26it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 200/200 [00:00<00:00, 1774.29ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:04<00:00,  4.63s/it]\n"
     ]
    }
   ],
   "source": [
    "chunk_size = 100_000\n",
    "for i, chunk_ds in enumerate(ds.iter(batch_size=chunk_size)):\n",
    "    chunk_regular = Dataset.from_dict(chunk_ds)\n",
    "\n",
    "    chunk_regular.push_to_hub(\n",
    "        repo_id=repo_id,\n",
    "        data_dir=f\"chunks/chunk_{i}\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"allenai/c4\", \"en\", split=\"train\", streaming=True)\n",
    "ds = ds.remove_columns([\"text\", \"timestamp\"])\n",
    "ds = ds.map(process_data, batched=True, batch_size=1_000)\n",
    "\n",
    "api = HfApi()\n",
    "repo_id = \"nhagar/test-upload-c4\"\n",
    "api.create_repo(repo_id, exist_ok=True, repo_type=\"dataset\")\n",
    "\n",
    "chunk_size = 1_000_000\n",
    "for i, chunk_ds in enumerate(ds.iter(batch_size=chunk_size)):\n",
    "    chunk_regular = Dataset.from_dict(chunk_ds)\n",
    "\n",
    "    chunk_regular.push_to_hub(\n",
    "        repo_id=repo_id,\n",
    "        data_dir=f\"chunks/chunk_{i}\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'url': 'https://klyq.com/beginners-bbq-class-taking-place-in-missoula/', 'tld': 'klyq.com'}\n",
      "Batch 0: Total examples so far: 62\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Total examples so far: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_examples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m \u001b[43mprocessed_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mextend(example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     18\u001b[0m processed_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtld\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mextend(example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtld\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(processed_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500_000\u001b[39m:\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "# Initialize a counter for total number of examples processed\n",
    "total_examples = 0\n",
    "i = 0\n",
    "\n",
    "processed_data = []\n",
    "\n",
    "# Loop through dataset until we reach or exceed 1 million examples\n",
    "for i, example in enumerate(ds):\n",
    "    print(example)\n",
    "    # Process each example\n",
    "    total_examples += len(example['url'])\n",
    "    \n",
    "    # Print progress every 100,000 examples\n",
    "    if i % 10000 == 0:\n",
    "        print(f\"Batch {i}: Total examples so far: {total_examples}\")\n",
    "\n",
    "    processed_data[\"url\"].extend(example[\"url\"])\n",
    "    processed_data[\"tld\"].extend(example[\"tld\"])\n",
    "\n",
    "    if len(processed_data[\"url\"]) >= 500_000:\n",
    "        print(\"Dumping data\")\n",
    "        processed_data = []\n",
    "    \n",
    "    # Break once we've reached 1 million examples\n",
    "    if total_examples >= 10:\n",
    "        print(f\"Reached target: {total_examples} examples processed in {i+1} batches\")\n",
    "        break\n",
    "\n",
    "print(f\"Final count: {total_examples} examples across {i+1} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'url': 'https://worldfinance100.com/2013/intelec-holdings/',\n",
       " 'tld': 'worldfinance100.com'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'IterableDataset' object has no attribute 'push_to_hub'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m ds \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39mremove_columns([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      3\u001b[0m ds \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39mmap(process_data, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10_000\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpush_to_hub\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnhagar/proc-test-c4\u001b[39m\u001b[38;5;124m\"\u001b[39m, private\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'IterableDataset' object has no attribute 'push_to_hub'"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"allenai/c4\", \"en\", split=\"train\", streaming=True)\n",
    "ds = ds.remove_columns([\"text\", \"timestamp\"])\n",
    "ds = ds.map(process_data, batched=True, batch_size=10_000)\n",
    "ds.push_to_hub(\"nhagar/proc-test-c4\", private=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
