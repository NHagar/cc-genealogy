{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "932bc267-ea7e-44fd-b649-348d057961c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>label</th>\n",
       "      <th>label_source</th>\n",
       "      <th>set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10167</th>\n",
       "      <td>tricountysentry.com</td>\n",
       "      <td>News</td>\n",
       "      <td>northeastern_domain_demo</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11414</th>\n",
       "      <td>redherring.com</td>\n",
       "      <td>News</td>\n",
       "      <td>northeastern_domain_demo</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11855</th>\n",
       "      <td>thesyrinx.com</td>\n",
       "      <td>News</td>\n",
       "      <td>northeastern_domain_demo</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7348</th>\n",
       "      <td>herald-publishing.com</td>\n",
       "      <td>News</td>\n",
       "      <td>northeastern_domain_demo</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11651</th>\n",
       "      <td>surpriseindependent.com</td>\n",
       "      <td>News</td>\n",
       "      <td>northeastern_domain_demo</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8046</th>\n",
       "      <td>laprensanwa.com</td>\n",
       "      <td>News</td>\n",
       "      <td>northeastern_domain_demo</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3443</th>\n",
       "      <td>best-car-lease-deals.co.uk</td>\n",
       "      <td>Entertainment &amp; Culture</td>\n",
       "      <td>data_provenance_init</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>benzinga.com</td>\n",
       "      <td>News</td>\n",
       "      <td>northeastern_domain_demo</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1967</th>\n",
       "      <td>assetverification.com</td>\n",
       "      <td>Business &amp; E-Commerce</td>\n",
       "      <td>data_provenance_init</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11367</th>\n",
       "      <td>patriotpost.us</td>\n",
       "      <td>News</td>\n",
       "      <td>northeastern_domain_demo</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           domain                    label  \\\n",
       "10167         tricountysentry.com                     News   \n",
       "11414              redherring.com                     News   \n",
       "11855               thesyrinx.com                     News   \n",
       "7348        herald-publishing.com                     News   \n",
       "11651     surpriseindependent.com                     News   \n",
       "8046              laprensanwa.com                     News   \n",
       "3443   best-car-lease-deals.co.uk  Entertainment & Culture   \n",
       "571                  benzinga.com                     News   \n",
       "1967        assetverification.com    Business & E-Commerce   \n",
       "11367              patriotpost.us                     News   \n",
       "\n",
       "                   label_source    set  \n",
       "10167  northeastern_domain_demo  train  \n",
       "11414  northeastern_domain_demo  train  \n",
       "11855  northeastern_domain_demo  train  \n",
       "7348   northeastern_domain_demo   test  \n",
       "11651  northeastern_domain_demo  train  \n",
       "8046   northeastern_domain_demo  train  \n",
       "3443       data_provenance_init  train  \n",
       "571    northeastern_domain_demo  train  \n",
       "1967       data_provenance_init  train  \n",
       "11367  northeastern_domain_demo  train  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download, list_repo_files\n",
    "from tqdm import tqdm\n",
    "import dask.dataframe as dd\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "labels = pd.read_csv('../data/combined_domain_labels_16k_splits.csv')\n",
    "labels.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4831d30-3a8b-43e8-9f0d-812952b500d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "datasets = {\n",
    "    #\"zyda_main\": \"hf://datasets/nhagar/zyda_urls/**/*.parquet\",\n",
    "    \"zyda_fwe3\": \"hf://datasets/nhagar/zyda-2_urls_fwe3/**/*.parquet\",\n",
    "    \"zyda_dclm_crossdeduped\": \"hf://datasets/nhagar/zyda-2_urls_dclm_crossdeduped/**/*.parquet\",\n",
    "    \"dclm_baseline_batch4\": \"hf://datasets/nhagar/dclm-baseline-1.0-parquet_urls/batch_4/train-*.parquet\",\n",
    "    \"dclm_dedup\": \"hf://datasets/nhagar/dclm-dedup_urls/**/*.parquet\",\n",
    "    \"falcon_refinedweb\": \"hf://datasets/nhagar/falcon-refinedweb_urls/batch*/train-*.parquet\",\n",
    "    \"falcon_main\": \"hf://datasets/nhagar/falcon_urls/data/train-*.parquet\",\n",
    "    \"c4_en\": \"hf://datasets/nhagar/c4_en_urls/data/train-*.parquet\",\n",
    "    \"cultura\": \"hf://datasets/nhagar/cultura_urls/data/train-*.parquet\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c42b9f9-0f74-425b-839e-3806827a75eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 parquet files for nhagar/zyda-2_urls_zyda_crossdeduped-filtered\n",
      "Downloading all parquet files...\n",
      "Downloaded 1 files\n",
      "First file path: hf_cache/datasets--nhagar--zyda-2_urls_zyda_crossdeduped-filtered/snapshots/695209cf7133a596fc999304fa623e802439281f/batch_1.parquet\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:07<00:00,  7.97s/it]\n"
     ]
    }
   ],
   "source": [
    "DATASETS = [\"nhagar/zyda-2_urls_zyda_crossdeduped-filtered\",\n",
    "            #\"nhagar/falcon_urls\"]\n",
    "           ]\n",
    "\n",
    "           \n",
    "for dataset in tqdm(DATASETS):\n",
    "    try:\n",
    "        # Get files list from repo\n",
    "        files = [f for f in list_repo_files(dataset, repo_type=\"dataset\") \n",
    "                if f.endswith('.parquet')]\n",
    "        \n",
    "        if not files:\n",
    "            print(f\"No parquet files found for {dataset}, skipping\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Found {len(files)} parquet files for {dataset}\")\n",
    "        print(f\"Downloading all parquet files...\")\n",
    "\n",
    "        downloaded_files = []\n",
    "        \n",
    "        for file in files:\n",
    "            download_path = hf_hub_download(\n",
    "                repo_id=dataset,\n",
    "                filename=file,\n",
    "                repo_type=\"dataset\",\n",
    "                cache_dir=\"hf_cache\"\n",
    "            )\n",
    "            downloaded_files.append(download_path)\n",
    "        \n",
    "        print(f\"Downloaded {len(downloaded_files)} files\")\n",
    "        print(f\"First file path: {downloaded_files[0]}\")\n",
    "        \n",
    "        # Use the actual downloaded paths directly\n",
    "        print(\"Processing...\")\n",
    "        df = dd.read_parquet(downloaded_files).compute()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error with {dataset}: {str(e)}\")\n",
    "        # Save progress on error\n",
    "        dataset_name = dataset.replace('nhagar/','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b423731-2775-4e53-a9f6-5cd1880c260b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>domain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.hennsnoxlaw.com/faqs</td>\n",
       "      <td>hennsnoxlaw.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://store.basscentral.com/dingwall/dingwal...</td>\n",
       "      <td>basscentral.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://theplayfullife.polarnopyretusa.com/name...</td>\n",
       "      <td>polarnopyretusa.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://www.katephillipsevents.com/contact</td>\n",
       "      <td>katephillipsevents.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.littleroomunderthestairs.com/2015/...</td>\n",
       "      <td>littleroomunderthestairs.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0                   https://www.hennsnoxlaw.com/faqs   \n",
       "1  https://store.basscentral.com/dingwall/dingwal...   \n",
       "2  http://theplayfullife.polarnopyretusa.com/name...   \n",
       "3          http://www.katephillipsevents.com/contact   \n",
       "4  https://www.littleroomunderthestairs.com/2015/...   \n",
       "\n",
       "                         domain  \n",
       "0               hennsnoxlaw.com  \n",
       "1               basscentral.com  \n",
       "2           polarnopyretusa.com  \n",
       "3        katephillipsevents.com  \n",
       "4  littleroomunderthestairs.com  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter to labeled domains\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c3998cf-2ced-4c93-83db-0ca36a350adb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40e56a3fdf6442969497030bd9b79105",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering domains:   0%|          | 0/1912 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def filter_with_progress(df, domain_set, batch_size=100000):\n",
    "    start_time = time.time()\n",
    "    total_rows = len(df)\n",
    "    filtered_rows = []\n",
    "    \n",
    "    for i in tqdm(range(0, total_rows, batch_size), desc=\"Filtering domains\"):\n",
    "        batch = df.iloc[i:min(i+batch_size, total_rows)]\n",
    "        filtered_batch = batch[batch['domain'].isin(domain_set)]\n",
    "        filtered_rows.append(filtered_batch)\n",
    "        \n",
    "        # Show additional progress info\n",
    "        if (i + batch_size) % (batch_size * 10) == 0 or (i + batch_size) >= total_rows:\n",
    "            elapsed = time.time() - start_time\n",
    "            #print(f\"Processed {min(i+batch_size, total_rows)}/{total_rows} rows ({(min(i+batch_size, total_rows)/total_rows)*100:.1f}%) in {elapsed:.1f}s\")\n",
    "    \n",
    "    return pd.concat(filtered_rows, ignore_index=True)\n",
    "\n",
    "# Use the function\n",
    "domain_set = set(labels[labels.set=='train']['domain'])\n",
    "filtered_df = filter_with_progress(df, domain_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e7957c8-0707-48df-8d23-fcfcf44da5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 3 urls from each domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e8da915c-5372-4774-8e79-02d3a886565b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000/14256 domains\n",
      "Processed 2000/14256 domains\n",
      "Processed 3000/14256 domains\n",
      "Processed 4000/14256 domains\n",
      "Processed 5000/14256 domains\n",
      "Processed 6000/14256 domains\n",
      "Processed 7000/14256 domains\n",
      "Processed 8000/14256 domains\n",
      "Processed 9000/14256 domains\n",
      "Processed 10000/14256 domains\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "result_data = []\n",
    "processed_domains = set()\n",
    "\n",
    "# Group and process in one pass\n",
    "for domain, group in filtered_df.groupby('domain'):\n",
    "    if domain not in domain_set:\n",
    "        continue\n",
    "        \n",
    "    urls = group['url'].head(3).tolist()\n",
    "    result_data.extend([(domain, url) for url in urls])\n",
    "    processed_domains.add(domain)\n",
    "    \n",
    "    if len(processed_domains) % 1000 == 0:\n",
    "        print(f\"Processed {len(processed_domains)}/{len(domain_set)} domains\")\n",
    "    \n",
    "    # Stop if we have all domains\n",
    "    if len(processed_domains) == len(domain_set):\n",
    "        break\n",
    "\n",
    "url_df = pd.DataFrame(result_data, columns=['domain', 'url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8b86f850-b886-42a2-87a5-7da38b4b359a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>url</th>\n",
       "      <th>label</th>\n",
       "      <th>label_source</th>\n",
       "      <th>set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10594</th>\n",
       "      <td>heal.com</td>\n",
       "      <td>https://heal.com/healthrecords/</td>\n",
       "      <td>Science, Academia, &amp; Technology</td>\n",
       "      <td>data_provenance_init</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13068</th>\n",
       "      <td>kiowacountypress.net</td>\n",
       "      <td>https://kiowacountypress.net/content/cdot-hold...</td>\n",
       "      <td>News</td>\n",
       "      <td>northeastern_domain_demo</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8378</th>\n",
       "      <td>fbnewsleader.com</td>\n",
       "      <td>https://www.fbnewsleader.com/news/coyote-sight...</td>\n",
       "      <td>News</td>\n",
       "      <td>northeastern_domain_demo</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29809</th>\n",
       "      <td>westword.com</td>\n",
       "      <td>https://www.westword.com/best-of/2019/shopping...</td>\n",
       "      <td>News</td>\n",
       "      <td>northeastern_domain_demo</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24644</th>\n",
       "      <td>tecumsehchieftain.com</td>\n",
       "      <td>https://www.tecumsehchieftain.com/articles/new...</td>\n",
       "      <td>News</td>\n",
       "      <td>northeastern_domain_demo</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22272</th>\n",
       "      <td>sanpedronewspilot.com</td>\n",
       "      <td>http://sanpedronewspilot.com/profiles/blogs/20...</td>\n",
       "      <td>News</td>\n",
       "      <td>northeastern_domain_demo</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30142</th>\n",
       "      <td>wicd15.com</td>\n",
       "      <td>http://wicd15.com/template/cgi-bin/archived.pl...</td>\n",
       "      <td>News</td>\n",
       "      <td>northeastern_domain_demo</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9408</th>\n",
       "      <td>gasconadecountyrepublican.com</td>\n",
       "      <td>http://gasconadecountyrepublican.com/content/g...</td>\n",
       "      <td>News</td>\n",
       "      <td>northeastern_domain_demo</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6145</th>\n",
       "      <td>dailynexus.com</td>\n",
       "      <td>http://dailynexus.com/2019-04-12/rowan-blasts-...</td>\n",
       "      <td>News</td>\n",
       "      <td>northeastern_domain_demo</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26116</th>\n",
       "      <td>themonitor.net</td>\n",
       "      <td>https://www.themonitor.net/article/traffic-sto...</td>\n",
       "      <td>News</td>\n",
       "      <td>northeastern_domain_demo</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              domain  \\\n",
       "10594                       heal.com   \n",
       "13068           kiowacountypress.net   \n",
       "8378                fbnewsleader.com   \n",
       "29809                   westword.com   \n",
       "24644          tecumsehchieftain.com   \n",
       "22272          sanpedronewspilot.com   \n",
       "30142                     wicd15.com   \n",
       "9408   gasconadecountyrepublican.com   \n",
       "6145                  dailynexus.com   \n",
       "26116                 themonitor.net   \n",
       "\n",
       "                                                     url  \\\n",
       "10594                    https://heal.com/healthrecords/   \n",
       "13068  https://kiowacountypress.net/content/cdot-hold...   \n",
       "8378   https://www.fbnewsleader.com/news/coyote-sight...   \n",
       "29809  https://www.westword.com/best-of/2019/shopping...   \n",
       "24644  https://www.tecumsehchieftain.com/articles/new...   \n",
       "22272  http://sanpedronewspilot.com/profiles/blogs/20...   \n",
       "30142  http://wicd15.com/template/cgi-bin/archived.pl...   \n",
       "9408   http://gasconadecountyrepublican.com/content/g...   \n",
       "6145   http://dailynexus.com/2019-04-12/rowan-blasts-...   \n",
       "26116  https://www.themonitor.net/article/traffic-sto...   \n",
       "\n",
       "                                 label              label_source    set  \n",
       "10594  Science, Academia, & Technology      data_provenance_init  train  \n",
       "13068                             News  northeastern_domain_demo  train  \n",
       "8378                              News  northeastern_domain_demo  train  \n",
       "29809                             News  northeastern_domain_demo  train  \n",
       "24644                             News  northeastern_domain_demo  train  \n",
       "22272                             News  northeastern_domain_demo  train  \n",
       "30142                             News  northeastern_domain_demo  train  \n",
       "9408                              News  northeastern_domain_demo  train  \n",
       "6145                              News  northeastern_domain_demo  train  \n",
       "26116                             News  northeastern_domain_demo  train  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract features\n",
    "url_df=url_df.merge(right=labels, left_on=\"domain\", right_on=\"domain\")\n",
    "url_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e90854-e92d-4de1-bed5-b8b3b200865a",
   "metadata": {},
   "source": [
    "# Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "eaaa1e2a-6900-49c6-b05a-b7c1f722a80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "from urllib.parse import urlparse\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, fbeta_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from openai import OpenAI\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "def extract_url_features(url):\n",
    "    parsed = urlparse(url)\n",
    "    domain = parsed.netloc\n",
    "    path = parsed.path\n",
    "    query = parsed.query\n",
    "\n",
    "    features = {\n",
    "        'url_length': len(url),\n",
    "        'domain_length': len(domain),\n",
    "        'path_length': len(path),\n",
    "        'num_slashes': url.count('/'),\n",
    "        'num_dots': url.count('.'),\n",
    "        'num_equal': url.count('='),\n",
    "        'num_params': len(query.split('&')) if query else 0,\n",
    "        'has_https': int(url.startswith('https')),\n",
    "        'has_www': int('www.' in domain),\n",
    "        'num_digits': sum(c.isdigit() for c in url),\n",
    "        'num_path_tokens': len([p for p in path.split('/') if p]),\n",
    "        'has_news_in_domain': int('news' in domain.lower()),\n",
    "        'has_news_in_path': int('news' in path.lower()),\n",
    "        'has_article_in_path': int('article' in path.lower()),\n",
    "        'has_content_in_path': int('content' in path.lower()),\n",
    "        'has_story_in_path': int('story' in path.lower()),\n",
    "        'has_date_pattern': int(bool(re.search(r'/(19|20)\\d{2}[-/](0[1-9]|1[0-2])[-/](0[1-9]|[12][0-9]|3[01])/', url))),\n",
    "        'has_blog_in_url': int('blog' in url.lower()),\n",
    "        'num_hyphens': url.count('-'),\n",
    "        'num_underscores': url.count('_'),\n",
    "        'has_query_string': int(bool(query)),\n",
    "        'has_ip_address': int(bool(re.match(r'(\\d{1,3}\\.){3}\\d{1,3}', domain))),\n",
    "        #'tld': domain.split('.')[-1] if '.' in domain else '',\n",
    "        'tld_length': len(domain.split('.')[-1]) if '.' in domain else 0,\n",
    "        'subdomain_depth': domain.count('.') - 1,\n",
    "        'is_shortened_url': int(domain in ['bit.ly', 'tinyurl.com', 'goo.gl', 'ow.ly', 't.co']),\n",
    "        'has_port': int(bool(parsed.port)),\n",
    "        'port_number': parsed.port if parsed.port else 0,\n",
    "        'has_fragment': int(bool(parsed.fragment)),\n",
    "        'fragment_length': len(parsed.fragment),\n",
    "        'path_digit_ratio': sum(c.isdigit() for c in path) / len(path) if len(path) > 0 else 0,\n",
    "        'domain_digit_ratio': sum(c.isdigit() for c in domain) / len(domain) if len(domain) > 0 else 0,\n",
    "        'query_length': len(query),\n",
    "        'num_semicolons': url.count(';'),\n",
    "        'num_at_symbols': url.count('@'),\n",
    "        'num_percent': url.count('%'),\n",
    "    }\n",
    "\n",
    "    return pd.Series(features)\n",
    "\n",
    "# Function to evaluate a model\n",
    "def evaluate_model(name, model, X_train, X_test, y_train, y_test):\n",
    "    # Training\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # Prediction\n",
    "    start_time = time.time()\n",
    "    y_pred = model.predict(X_test)\n",
    "    inference_time = time.time() - start_time\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    f2 = fbeta_score(y_test, y_pred, beta=2, average='weighted')\n",
    "    \n",
    "    print(f\"\\n{name} Results:\")\n",
    "    print(f\"Training time: {train_time:.4f} seconds\")\n",
    "    print(f\"Inference time: {inference_time:.4f} seconds\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"F2 Score: {f2:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    return {\n",
    "        'Model': name,\n",
    "        'Train Time (s)': train_time,\n",
    "        'Inference Time (s)': inference_time,\n",
    "        'Accuracy': accuracy,\n",
    "        'F1 Score': f1,\n",
    "        'F2 Score': f2\n",
    "    }\n",
    "\n",
    "\n",
    "def classify_url_with_llm(url, model, prompt):\n",
    "    llm = OpenAI(base_url=\"http://127.0.0.1:1234/v1\", api_key=\"lm-studio\")\n",
    "    \n",
    "    resp = llm.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": prompt},\n",
    "            {\"role\": \"user\", \"content\": url},\n",
    "        ],\n",
    "    )\n",
    "    txt = resp.choices[0].message.content\n",
    "\n",
    "    # Extract JSON output from the response\n",
    "    json_extract_pattern = re.compile(r\"```json\\n(.*?)\\n```\", re.DOTALL)\n",
    "    json_extract = json_extract_pattern.search(txt).group(1)\n",
    "\n",
    "    return json.loads(json_extract)\n",
    "\n",
    "\n",
    "\n",
    "def classify_urls(url_df):\n",
    "    print(\"Starting URL Classification for News Detection...\")\n",
    "    \n",
    "    # Convert multi-class labels to binary (news or not_news)\n",
    "    url_df[\"binary_label\"] = url_df[\"label\"].apply(lambda x: \"news\" if x == \"News\" else \"not_news\")\n",
    "    \n",
    "    # Check data\n",
    "    print(f\"Total samples: {len(url_df)}\")\n",
    "    print(f\"News samples: {sum(url_df['binary_label'] == 'news')}\")\n",
    "    print(f\"Non-news samples: {sum(url_df['binary_label'] == 'not_news')}\")\n",
    "    \n",
    "    # Split data if no test set defined\n",
    "    if 'set' not in url_df.columns or url_df['set'].isna().any():\n",
    "        print(\"Creating train/test split...\")\n",
    "        train_df, test_df = train_test_split(url_df, test_size=0.2, stratify=url_df['binary_label'], random_state=42)\n",
    "    else:\n",
    "        train_df = url_df[url_df['set'] == 'train']\n",
    "        test_df = url_df[url_df['set'] == 'test']\n",
    "        if len(test_df) == 0:  # If no test set exists\n",
    "            print(\"No test set found, creating from train set...\")\n",
    "            train_df, test_df = train_test_split(train_df, test_size=0.2, stratify=train_df['binary_label'], random_state=42)\n",
    "    \n",
    "    print(f\"Training samples: {len(train_df)}\")\n",
    "    print(f\"Testing samples: {len(test_df)}\")\n",
    "    \n",
    "    # Prepare labels\n",
    "    y_train = train_df['binary_label']\n",
    "    y_test = test_df['binary_label']\n",
    "    \n",
    "    # Store results\n",
    "    results = []\n",
    "    \n",
    "    # Approach 1: Lexical Features\n",
    "    print(\"\\n=== Approach 1: Using Lexical Features ===\")\n",
    "    X_train_lex = train_df['url'].apply(extract_url_features)\n",
    "    X_test_lex = test_df['url'].apply(extract_url_features)\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_lex_scaled = scaler.fit_transform(X_train_lex)\n",
    "    X_test_lex_scaled = scaler.transform(X_test_lex)\n",
    "    \n",
    "    # Models\n",
    "    dt_lex = DecisionTreeClassifier(max_depth=10, random_state=42)\n",
    "    knn_lex = KNeighborsClassifier(n_neighbors=5)\n",
    "    rf_lex = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    \n",
    "    # Evaluate\n",
    "    results.append(evaluate_model(\"Decision Tree (Lexical)\", dt_lex, X_train_lex_scaled, X_test_lex_scaled, y_train, y_test))\n",
    "    results.append(evaluate_model(\"KNN (Lexical)\", knn_lex, X_train_lex_scaled, X_test_lex_scaled, y_train, y_test))\n",
    "    results.append(evaluate_model(\"Random Forest (Lexical)\", rf_lex, X_train_lex_scaled, X_test_lex_scaled, y_train, y_test))\n",
    "\n",
    "    \n",
    "    # Approach 2: Character N-grams\n",
    "    print(\"\\n=== Approach 2: Using Character N-grams ===\")\n",
    "    char_vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(3, 5), max_features=1000)\n",
    "    X_train_char = char_vectorizer.fit_transform(train_df['url'])\n",
    "    X_test_char = char_vectorizer.transform(test_df['url'])\n",
    "    \n",
    "    # Models\n",
    "    dt_char = DecisionTreeClassifier(max_depth=20, random_state=42)\n",
    "    knn_char = KNeighborsClassifier(n_neighbors=5)\n",
    "    rf_char = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    \n",
    "    # Evaluate\n",
    "    results.append(evaluate_model(\"Decision Tree (Char N-grams)\", dt_char, X_train_char, X_test_char, y_train, y_test))\n",
    "    results.append(evaluate_model(\"KNN (Char N-grams)\", knn_char, X_train_char, X_test_char, y_train, y_test))\n",
    "    results.append(evaluate_model(\"Random Forest (Char N-grams)\", rf_char, X_train_char, X_test_char, y_train, y_test))\n",
    "    \n",
    "    # Approach 3: Domain Features\n",
    "    print(\"\\n=== Approach 3: Using Domain-Specific Features ===\")\n",
    "    # Extract URL parts\n",
    "    train_df['domain'] = train_df['url'].apply(lambda x: urlparse(x).netloc)\n",
    "    test_df['domain'] = test_df['url'].apply(lambda x: urlparse(x).netloc)\n",
    "    train_df['path'] = train_df['url'].apply(lambda x: urlparse(x).path)\n",
    "    test_df['path'] = test_df['url'].apply(lambda x: urlparse(x).path)\n",
    "    \n",
    "    # Create TF-IDF features for domains\n",
    "    domain_vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(2, 5), max_features=500)\n",
    "    X_train_domain = domain_vectorizer.fit_transform(train_df['domain'])\n",
    "    X_test_domain = domain_vectorizer.transform(test_df['domain'])\n",
    "    \n",
    "    # Create TF-IDF features for paths\n",
    "    path_vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1, 5), max_features=500, token_pattern=r'[a-zA-Z0-9]+')\n",
    "    X_train_path = path_vectorizer.fit_transform(train_df['path'])\n",
    "    X_test_path = path_vectorizer.transform(test_df['path'])\n",
    "    \n",
    "    # Combine sparse matrices\n",
    "    from scipy.sparse import hstack\n",
    "    X_train_combined = hstack([X_train_domain, X_train_path])\n",
    "    X_test_combined = hstack([X_test_domain, X_test_path])\n",
    "    \n",
    "    # Models\n",
    "    dt_combined = DecisionTreeClassifier(max_depth=100, random_state=42)\n",
    "    knn_combined = KNeighborsClassifier(n_neighbors=5)\n",
    "    rf_combined = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    \n",
    "    # Evaluate\n",
    "    results.append(evaluate_model(\"Decision Tree (Domain+Path)\", dt_combined, X_train_combined, X_test_combined, y_train, y_test))\n",
    "    results.append(evaluate_model(\"KNN (Domain+Path)\", knn_combined, X_train_combined, X_test_combined, y_train, y_test))\n",
    "    results.append(evaluate_model(\"Random Forest (Domain+Path)\", rf_combined, X_train_combined, X_test_combined, y_train, y_test))\n",
    "    \n",
    "    # Approach 4: Using LLM Classification\n",
    "    print(\"\\n=== Approach 4: Using LLM Classification ===\")\n",
    "    \n",
    "    # Load prompts\n",
    "    with open('prompt_is_news.txt', 'r') as file:\n",
    "        prompt_binary_label = file.read()\n",
    "\n",
    "    \n",
    "    # Sample a small subset of test data for LLM evaluation (LLMs are slower)\n",
    "    llm_test_size = min(1000, len(test_df))\n",
    "    llm_test_df = test_df.sample(llm_test_size, random_state=42)\n",
    "    \n",
    "    # List of models to evaluate\n",
    "    llm_models = [\n",
    "        \"gemma-2-9b-it-GGUF\",\n",
    "    ]\n",
    "    \n",
    "    for model in llm_models:\n",
    "        print(f\"\\nProcessing LLM model: {model}\")\n",
    "        \n",
    "        # Time the binary classification\n",
    "        print(f\"Starting binary classification for {model}...\")\n",
    "        start_time = time.time()\n",
    "        llm_test_df[f\"{model}_binary_label\"] = llm_test_df[\"url\"].apply(\n",
    "            classify_url_with_llm, model=model, prompt=prompt_binary_label\n",
    "        )\n",
    "        binary_time = time.time() - start_time\n",
    "        print(f\"  Completed binary classification in {binary_time:.2f}s ({binary_time/len(llm_test_df):.4f}s per URL)\")\n",
    "        \n",
    "        # Extract binary predictions\n",
    "        llm_test_df[f\"{model}_is_news\"] = llm_test_df[f\"{model}_binary_label\"].apply(\n",
    "            lambda x: x[\"is_news\"]\n",
    "        )\n",
    "        \n",
    "        # Convert to correct format for evaluation (0/1 -> news/not_news)\n",
    "        llm_test_df[f\"{model}_pred\"] = llm_test_df[f\"{model}_is_news\"].apply(\n",
    "            lambda x: \"news\" if x == 1 else \"not_news\"\n",
    "        )\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(llm_test_df[\"binary_label\"], llm_test_df[f\"{model}_pred\"])\n",
    "        f1 = f1_score(llm_test_df[\"binary_label\"], llm_test_df[f\"{model}_pred\"], pos_label=\"news\", average=\"binary\")\n",
    "        f2 = fbeta_score(llm_test_df[\"binary_label\"], llm_test_df[f\"{model}_pred\"], beta=2, pos_label=\"news\", average=\"binary\")\n",
    "        \n",
    "        # Add to results\n",
    "        results.append({\n",
    "            'Model': f\"{model} (LLM)\",\n",
    "            'Train Time (s)': binary_time,  # Total time spent on inference\n",
    "            'Inference Time (s)': binary_time / len(llm_test_df),  # Average time per URL\n",
    "            'Accuracy': accuracy,\n",
    "            'F1 Score': f1,\n",
    "            'F2 Score': f2\n",
    "        })\n",
    "        \n",
    "        print(f\"\\n{model} (LLM) Results:\")\n",
    "        print(f\"Processing time: {binary_time:.4f} seconds\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"F1 Score: {f1:.4f}\")\n",
    "        print(f\"F2 Score: {f2:.4f}\")\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(llm_test_df[\"binary_label\"], llm_test_df[f\"{model}_pred\"]))\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n=== Summary of Results ===\")\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(results_df.sort_values('Accuracy', ascending=False))\n",
    "    \n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d0b9fda9-4db5-4c29-8884-c4a3f7077289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting URL Classification for News Detection...\n",
      "Total samples: 31598\n",
      "News samples: 26899\n",
      "Non-news samples: 4699\n",
      "No test set found, creating from train set...\n",
      "Training samples: 25278\n",
      "Testing samples: 6320\n",
      "\n",
      "=== Approach 1: Using Lexical Features ===\n",
      "\n",
      "Decision Tree (Lexical) Results:\n",
      "Training time: 0.0535 seconds\n",
      "Inference time: 0.0005 seconds\n",
      "Accuracy: 0.8634\n",
      "F1 Score: 0.8377\n",
      "F2 Score: 0.8514\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        news       0.88      0.97      0.92      5380\n",
      "    not_news       0.60      0.24      0.34       940\n",
      "\n",
      "    accuracy                           0.86      6320\n",
      "   macro avg       0.74      0.61      0.63      6320\n",
      "weighted avg       0.84      0.86      0.84      6320\n",
      "\n",
      "\n",
      "KNN (Lexical) Results:\n",
      "Training time: 0.0098 seconds\n",
      "Inference time: 0.1469 seconds\n",
      "Accuracy: 0.8646\n",
      "F1 Score: 0.8542\n",
      "F2 Score: 0.8599\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        news       0.90      0.95      0.92      5380\n",
      "    not_news       0.56      0.39      0.46       940\n",
      "\n",
      "    accuracy                           0.86      6320\n",
      "   macro avg       0.73      0.67      0.69      6320\n",
      "weighted avg       0.85      0.86      0.85      6320\n",
      "\n",
      "\n",
      "Random Forest (Lexical) Results:\n",
      "Training time: 1.3270 seconds\n",
      "Inference time: 0.0728 seconds\n",
      "Accuracy: 0.8748\n",
      "F1 Score: 0.8617\n",
      "F2 Score: 0.8687\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        news       0.90      0.96      0.93      5380\n",
      "    not_news       0.63      0.38      0.48       940\n",
      "\n",
      "    accuracy                           0.87      6320\n",
      "   macro avg       0.76      0.67      0.70      6320\n",
      "weighted avg       0.86      0.87      0.86      6320\n",
      "\n",
      "\n",
      "=== Approach 2: Using Character N-grams ===\n",
      "\n",
      "Decision Tree (Char N-grams) Results:\n",
      "Training time: 3.4106 seconds\n",
      "Inference time: 0.0049 seconds\n",
      "Accuracy: 0.8563\n",
      "F1 Score: 0.8423\n",
      "F2 Score: 0.8500\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        news       0.89      0.95      0.92      5380\n",
      "    not_news       0.53      0.33      0.41       940\n",
      "\n",
      "    accuracy                           0.86      6320\n",
      "   macro avg       0.71      0.64      0.66      6320\n",
      "weighted avg       0.84      0.86      0.84      6320\n",
      "\n",
      "\n",
      "KNN (Char N-grams) Results:\n",
      "Training time: 0.0112 seconds\n",
      "Inference time: 53.6409 seconds\n",
      "Accuracy: 0.8856\n",
      "F1 Score: 0.8843\n",
      "F2 Score: 0.8851\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        news       0.93      0.94      0.93      5380\n",
      "    not_news       0.62      0.59      0.60       940\n",
      "\n",
      "    accuracy                           0.89      6320\n",
      "   macro avg       0.78      0.76      0.77      6320\n",
      "weighted avg       0.88      0.89      0.88      6320\n",
      "\n",
      "\n",
      "Random Forest (Char N-grams) Results:\n",
      "Training time: 16.5902 seconds\n",
      "Inference time: 0.1061 seconds\n",
      "Accuracy: 0.8870\n",
      "F1 Score: 0.8709\n",
      "F2 Score: 0.8790\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        news       0.90      0.98      0.94      5380\n",
      "    not_news       0.74      0.37      0.50       940\n",
      "\n",
      "    accuracy                           0.89      6320\n",
      "   macro avg       0.82      0.68      0.72      6320\n",
      "weighted avg       0.88      0.89      0.87      6320\n",
      "\n",
      "\n",
      "=== Approach 3: Using Domain-Specific Features ===\n",
      "\n",
      "Decision Tree (Domain+Path) Results:\n",
      "Training time: 2.7695 seconds\n",
      "Inference time: 0.0033 seconds\n",
      "Accuracy: 0.9468\n",
      "F1 Score: 0.9472\n",
      "F2 Score: 0.9470\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        news       0.97      0.97      0.97      5380\n",
      "    not_news       0.81      0.84      0.82       940\n",
      "\n",
      "    accuracy                           0.95      6320\n",
      "   macro avg       0.89      0.90      0.90      6320\n",
      "weighted avg       0.95      0.95      0.95      6320\n",
      "\n",
      "\n",
      "KNN (Domain+Path) Results:\n",
      "Training time: 0.0101 seconds\n",
      "Inference time: 12.4954 seconds\n",
      "Accuracy: 0.8696\n",
      "F1 Score: 0.8726\n",
      "F2 Score: 0.8707\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        news       0.93      0.91      0.92      5380\n",
      "    not_news       0.56      0.62      0.59       940\n",
      "\n",
      "    accuracy                           0.87      6320\n",
      "   macro avg       0.74      0.77      0.75      6320\n",
      "weighted avg       0.88      0.87      0.87      6320\n",
      "\n",
      "\n",
      "Random Forest (Domain+Path) Results:\n",
      "Training time: 7.6579 seconds\n",
      "Inference time: 0.0984 seconds\n",
      "Accuracy: 0.9764\n",
      "F1 Score: 0.9757\n",
      "F2 Score: 0.9759\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        news       0.97      1.00      0.99      5380\n",
      "    not_news       0.99      0.85      0.91       940\n",
      "\n",
      "    accuracy                           0.98      6320\n",
      "   macro avg       0.98      0.93      0.95      6320\n",
      "weighted avg       0.98      0.98      0.98      6320\n",
      "\n",
      "\n",
      "=== Approach 4: Using LLM Classification ===\n",
      "\n",
      "Processing LLM model: gemma-2-9b-it-GGUF\n",
      "Starting binary classification for gemma-2-9b-it-GGUF...\n",
      "  Completed binary classification in 6385.12s (6.3851s per URL)\n",
      "\n",
      "gemma-2-9b-it-GGUF (LLM) Results:\n",
      "Processing time: 6385.1157 seconds\n",
      "Accuracy: 0.8470\n",
      "F1 Score: 0.9016\n",
      "F2 Score: 0.8563\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        news       0.99      0.83      0.90       846\n",
      "    not_news       0.50      0.95      0.66       154\n",
      "\n",
      "    accuracy                           0.85      1000\n",
      "   macro avg       0.75      0.89      0.78      1000\n",
      "weighted avg       0.91      0.85      0.86      1000\n",
      "\n",
      "\n",
      "=== Summary of Results ===\n",
      "                          Model  Train Time (s)  Inference Time (s)  Accuracy  \\\n",
      "8   Random Forest (Domain+Path)        7.657918            0.098400  0.976424   \n",
      "6   Decision Tree (Domain+Path)        2.769504            0.003268  0.946835   \n",
      "5  Random Forest (Char N-grams)       16.590193            0.106071  0.887025   \n",
      "4            KNN (Char N-grams)        0.011227           53.640871  0.885601   \n",
      "2       Random Forest (Lexical)        1.326974            0.072770  0.874842   \n",
      "7             KNN (Domain+Path)        0.010121           12.495374  0.869620   \n",
      "1                 KNN (Lexical)        0.009754            0.146912  0.864557   \n",
      "0       Decision Tree (Lexical)        0.053504            0.000481  0.863449   \n",
      "3  Decision Tree (Char N-grams)        3.410615            0.004884  0.856329   \n",
      "9      gemma-2-9b-it-GGUF (LLM)     6385.115657            6.385116  0.847000   \n",
      "\n",
      "   F1 Score  F2 Score  \n",
      "8  0.975695  0.975946  \n",
      "6  0.947246  0.946987  \n",
      "5  0.870851  0.879010  \n",
      "4  0.884276  0.885051  \n",
      "2  0.861726  0.868674  \n",
      "7  0.872550  0.870703  \n",
      "1  0.854160  0.859859  \n",
      "0  0.837657  0.851373  \n",
      "3  0.842262  0.849970  \n",
      "9  0.901608  0.856340  \n",
      "\n",
      "=== Final Comparison ===\n",
      "                          Model  Train Time (s)  Inference Time (s)  Accuracy  \\\n",
      "8   Random Forest (Domain+Path)        7.657918            0.098400  0.976424   \n",
      "6   Decision Tree (Domain+Path)        2.769504            0.003268  0.946835   \n",
      "5  Random Forest (Char N-grams)       16.590193            0.106071  0.887025   \n",
      "4            KNN (Char N-grams)        0.011227           53.640871  0.885601   \n",
      "2       Random Forest (Lexical)        1.326974            0.072770  0.874842   \n",
      "7             KNN (Domain+Path)        0.010121           12.495374  0.869620   \n",
      "1                 KNN (Lexical)        0.009754            0.146912  0.864557   \n",
      "0       Decision Tree (Lexical)        0.053504            0.000481  0.863449   \n",
      "3  Decision Tree (Char N-grams)        3.410615            0.004884  0.856329   \n",
      "9      gemma-2-9b-it-GGUF (LLM)     6385.115657            6.385116  0.847000   \n",
      "\n",
      "   F1 Score  F2 Score  \n",
      "8  0.975695  0.975946  \n",
      "6  0.947246  0.946987  \n",
      "5  0.870851  0.879010  \n",
      "4  0.884276  0.885051  \n",
      "2  0.861726  0.868674  \n",
      "7  0.872550  0.870703  \n",
      "1  0.854160  0.859859  \n",
      "0  0.837657  0.851373  \n",
      "3  0.842262  0.849970  \n",
      "9  0.901608  0.856340  \n"
     ]
    }
   ],
   "source": [
    "# Run benchmarks\n",
    "url_df = url_df[[\"url\",\"domain\",\"label\",\"set\"]]\n",
    "url_df[\"binary_label\"] = url_df[\"label\"].apply(lambda x: \"news\" if x == \"News\" else \"not_news\")\n",
    "results_df = classify_urls(url_df)\n",
    "#results_df = run_benchmarks(df)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n=== Final Comparison ===\")\n",
    "print(results_df.sort_values('Accuracy', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "beed02ac-ae79-4574-bedd-7724479022e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Train Time (s)</th>\n",
       "      <th>Inference Time (s)</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>F2 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Decision Tree (Lexical)</td>\n",
       "      <td>0.053504</td>\n",
       "      <td>0.000481</td>\n",
       "      <td>0.863449</td>\n",
       "      <td>0.837657</td>\n",
       "      <td>0.851373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KNN (Lexical)</td>\n",
       "      <td>0.009754</td>\n",
       "      <td>0.146912</td>\n",
       "      <td>0.864557</td>\n",
       "      <td>0.854160</td>\n",
       "      <td>0.859859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest (Lexical)</td>\n",
       "      <td>1.326974</td>\n",
       "      <td>0.072770</td>\n",
       "      <td>0.874842</td>\n",
       "      <td>0.861726</td>\n",
       "      <td>0.868674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Decision Tree (Char N-grams)</td>\n",
       "      <td>3.410615</td>\n",
       "      <td>0.004884</td>\n",
       "      <td>0.856329</td>\n",
       "      <td>0.842262</td>\n",
       "      <td>0.849970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KNN (Char N-grams)</td>\n",
       "      <td>0.011227</td>\n",
       "      <td>53.640871</td>\n",
       "      <td>0.885601</td>\n",
       "      <td>0.884276</td>\n",
       "      <td>0.885051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Random Forest (Char N-grams)</td>\n",
       "      <td>16.590193</td>\n",
       "      <td>0.106071</td>\n",
       "      <td>0.887025</td>\n",
       "      <td>0.870851</td>\n",
       "      <td>0.879010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Decision Tree (Domain+Path)</td>\n",
       "      <td>2.769504</td>\n",
       "      <td>0.003268</td>\n",
       "      <td>0.946835</td>\n",
       "      <td>0.947246</td>\n",
       "      <td>0.946987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>KNN (Domain+Path)</td>\n",
       "      <td>0.010121</td>\n",
       "      <td>12.495374</td>\n",
       "      <td>0.869620</td>\n",
       "      <td>0.872550</td>\n",
       "      <td>0.870703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Random Forest (Domain+Path)</td>\n",
       "      <td>7.657918</td>\n",
       "      <td>0.098400</td>\n",
       "      <td>0.976424</td>\n",
       "      <td>0.975695</td>\n",
       "      <td>0.975946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>gemma-2-9b-it-GGUF (LLM)</td>\n",
       "      <td>6385.115657</td>\n",
       "      <td>6.385116</td>\n",
       "      <td>0.847000</td>\n",
       "      <td>0.901608</td>\n",
       "      <td>0.856340</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Model  Train Time (s)  Inference Time (s)  Accuracy  \\\n",
       "0       Decision Tree (Lexical)        0.053504            0.000481  0.863449   \n",
       "1                 KNN (Lexical)        0.009754            0.146912  0.864557   \n",
       "2       Random Forest (Lexical)        1.326974            0.072770  0.874842   \n",
       "3  Decision Tree (Char N-grams)        3.410615            0.004884  0.856329   \n",
       "4            KNN (Char N-grams)        0.011227           53.640871  0.885601   \n",
       "5  Random Forest (Char N-grams)       16.590193            0.106071  0.887025   \n",
       "6   Decision Tree (Domain+Path)        2.769504            0.003268  0.946835   \n",
       "7             KNN (Domain+Path)        0.010121           12.495374  0.869620   \n",
       "8   Random Forest (Domain+Path)        7.657918            0.098400  0.976424   \n",
       "9      gemma-2-9b-it-GGUF (LLM)     6385.115657            6.385116  0.847000   \n",
       "\n",
       "   F1 Score  F2 Score  \n",
       "0  0.837657  0.851373  \n",
       "1  0.854160  0.859859  \n",
       "2  0.861726  0.868674  \n",
       "3  0.842262  0.849970  \n",
       "4  0.884276  0.885051  \n",
       "5  0.870851  0.879010  \n",
       "6  0.947246  0.946987  \n",
       "7  0.872550  0.870703  \n",
       "8  0.975695  0.975946  \n",
       "9  0.901608  0.856340  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23e267e-dc44-4a72-b8dc-820a28cb30bb",
   "metadata": {},
   "source": [
    "# Multi-label Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "58e3c9b4-7b9b-4392-b5a7-d92cf0d43ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TREE_DEPTH = 100\n",
    "MAX_NGRAM_FEATURES = 1000\n",
    "MAX_ESTIMATORS_FOREST = 20\n",
    "N_NEIGHBORS = 3\n",
    "NGRAM_RANGE_MIN = 2\n",
    "NGRAM_RANGE_MAX = 6\n",
    "\n",
    "def classify_urls_multilabel(url_df):\n",
    "    print(\"Starting URL Classification with Multi-class Labels...\")\n",
    "    \n",
    "    # Check data\n",
    "    print(f\"Total samples: {len(url_df)}\")\n",
    "    print(\"Label distribution:\")\n",
    "    label_counts = url_df['label'].value_counts()\n",
    "    for label, count in label_counts.items():\n",
    "        print(f\"  {label}: {count} samples\")\n",
    "    \n",
    "    # Store the original multi-class label\n",
    "    url_df[\"original_label\"] = url_df[\"label\"]\n",
    "    \n",
    "    # Handle NaN labels if any exist\n",
    "    if url_df[\"label\"].isna().any():\n",
    "        print(f\"Warning: Found {url_df['label'].isna().sum()} NaN labels. Replacing with 'Other'.\")\n",
    "        url_df[\"label\"] = url_df[\"label\"].fillna(\"Other\")\n",
    "    \n",
    "    # Split data if no test set defined\n",
    "    if 'set' not in url_df.columns or url_df['set'].isna().any():\n",
    "        print(\"Creating train/test split...\")\n",
    "        train_df, test_df = train_test_split(url_df, test_size=0.2, stratify=url_df['label'], random_state=42)\n",
    "    else:\n",
    "        train_df = url_df[url_df['set'] == 'train']\n",
    "        test_df = url_df[url_df['set'] == 'test']\n",
    "        if len(test_df) == 0:  # If no test set exists\n",
    "            print(\"No test set found, creating from train set...\")\n",
    "            train_df, test_df = train_test_split(train_df, test_size=0.2, stratify=train_df['label'], random_state=42)\n",
    "    \n",
    "    print(f\"Training samples: {len(train_df)}\")\n",
    "    print(f\"Testing samples: {len(test_df)}\")\n",
    "    \n",
    "    # Prepare labels (using full multi-class labels)\n",
    "    y_train = train_df['label']\n",
    "    y_test = test_df['label']\n",
    "    \n",
    "    # Store results\n",
    "    results = []\n",
    "    \n",
    "    # Approach 1: Lexical Features\n",
    "    print(\"\\n=== Approach 1: Using Lexical Features ===\")\n",
    "    X_train_lex = train_df['url'].apply(extract_url_features)\n",
    "    X_test_lex = test_df['url'].apply(extract_url_features)\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_lex_scaled = scaler.fit_transform(X_train_lex)\n",
    "    X_test_lex_scaled = scaler.transform(X_test_lex)\n",
    "    \n",
    "    # Models\n",
    "    dt_lex = DecisionTreeClassifier(max_depth=MAX_TREE_DEPTH, random_state=42)\n",
    "    knn_lex = KNeighborsClassifier(n_neighbors=N_NEIGHBORS)\n",
    "    rf_lex = RandomForestClassifier(n_estimators=MAX_ESTIMATORS_FOREST, random_state=42)\n",
    "    \n",
    "    # Evaluate\n",
    "    results.append(evaluate_model(\"Decision Tree (Lexical)\", dt_lex, X_train_lex_scaled, X_test_lex_scaled, y_train, y_test))\n",
    "    results.append(evaluate_model(\"KNN (Lexical)\", knn_lex, X_train_lex_scaled, X_test_lex_scaled, y_train, y_test))\n",
    "    results.append(evaluate_model(\"Random Forest (Lexical)\", rf_lex, X_train_lex_scaled, X_test_lex_scaled, y_train, y_test))\n",
    "\n",
    "    # Print important features for Random Forest (Lexical)\n",
    "    print(\"\\n=== Most Important Lexical Features ===\")\n",
    "    rf_lex.fit(X_train_lex_scaled, y_train)\n",
    "    # Assuming X_train_lex has named columns or we can get feature names\n",
    "    feature_names_lex = X_train_lex.columns if hasattr(X_train_lex, 'columns') else [f\"lexical_feature_{i}\" for i in range(X_train_lex_scaled.shape[1])]\n",
    "    print_feature_importance(rf_lex, feature_names_lex, \"Random Forest - Lexical Features\")\n",
    "    \n",
    "    # Approach 2: Character N-grams\n",
    "    print(\"\\n=== Approach 2: Using Character N-grams ===\")\n",
    "    char_vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(NGRAM_RANGE_MIN, NGRAM_RANGE_MAX), max_features=MAX_NGRAM_FEATURES)\n",
    "    X_train_char = char_vectorizer.fit_transform(train_df['url'])\n",
    "    X_test_char = char_vectorizer.transform(test_df['url'])\n",
    "    \n",
    "    # Models\n",
    "    dt_char = DecisionTreeClassifier(max_depth=MAX_TREE_DEPTH, random_state=42)\n",
    "    knn_char = KNeighborsClassifier(n_neighbors=N_NEIGHBORS)\n",
    "    rf_char = RandomForestClassifier(n_estimators=MAX_ESTIMATORS_FOREST, random_state=42)\n",
    "    \n",
    "    # Evaluate\n",
    "    results.append(evaluate_model(\"Decision Tree (Char N-grams)\", dt_char, X_train_char, X_test_char, y_train, y_test))\n",
    "    results.append(evaluate_model(\"KNN (Char N-grams)\", knn_char, X_train_char, X_test_char, y_train, y_test))\n",
    "    results.append(evaluate_model(\"Random Forest (Char N-grams)\", rf_char, X_train_char, X_test_char, y_train, y_test))\n",
    "    \n",
    "    # Print important features for Random Forest (Char N-grams)\n",
    "    print(\"\\n=== Most Important Character N-gram Features ===\")\n",
    "    rf_char.fit(X_train_char, y_train)\n",
    "    feature_names_char = char_vectorizer.get_feature_names_out()\n",
    "    print_feature_importance(rf_char, feature_names_char, \"Random Forest - Character N-gram Features\")\n",
    "    \n",
    "    # Approach 3: Domain Features\n",
    "    print(\"\\n=== Approach 3: Using Domain-Specific Features ===\")\n",
    "    # Extract URL parts\n",
    "    train_df['domain'] = train_df['url'].apply(lambda x: urlparse(x).netloc)\n",
    "    test_df['domain'] = test_df['url'].apply(lambda x: urlparse(x).netloc)\n",
    "    train_df['path'] = train_df['url'].apply(lambda x: urlparse(x).path)\n",
    "    test_df['path'] = test_df['url'].apply(lambda x: urlparse(x).path)\n",
    "    \n",
    "    # Create TF-IDF features for domains\n",
    "    domain_vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(NGRAM_RANGE_MIN, NGRAM_RANGE_MAX), max_features=MAX_NGRAM_FEATURES)\n",
    "    X_train_domain = domain_vectorizer.fit_transform(train_df['domain'])\n",
    "    X_test_domain = domain_vectorizer.transform(test_df['domain'])\n",
    "    \n",
    "    # Create TF-IDF features for paths\n",
    "    path_vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(NGRAM_RANGE_MIN, NGRAM_RANGE_MAX), max_features=MAX_NGRAM_FEATURES, token_pattern=r'[a-zA-Z0-9]+')\n",
    "    X_train_path = path_vectorizer.fit_transform(train_df['path'])\n",
    "    X_test_path = path_vectorizer.transform(test_df['path'])\n",
    "    \n",
    "    # Combine sparse matrices\n",
    "    from scipy.sparse import hstack\n",
    "    X_train_combined = hstack([X_train_domain, X_train_path])\n",
    "    X_test_combined = hstack([X_test_domain, X_test_path])\n",
    "    \n",
    "    # Models\n",
    "    dt_combined = DecisionTreeClassifier(max_depth=MAX_TREE_DEPTH, random_state=42)\n",
    "    knn_combined = KNeighborsClassifier(n_neighbors=N_NEIGHBORS)\n",
    "    rf_combined = RandomForestClassifier(n_estimators=MAX_ESTIMATORS_FOREST, random_state=42)\n",
    "    \n",
    "    # Evaluate\n",
    "    results.append(evaluate_model(\"Decision Tree (Domain+Path)\", dt_combined, X_train_combined, X_test_combined, y_train, y_test))\n",
    "    results.append(evaluate_model(\"KNN (Domain+Path)\", knn_combined, X_train_combined, X_test_combined, y_train, y_test))\n",
    "    results.append(evaluate_model(\"Random Forest (Domain+Path)\", rf_combined, X_train_combined, X_test_combined, y_train, y_test))\n",
    "    \n",
    "    # Print important features for Random Forest (Domain+Path)\n",
    "    print(\"\\n=== Most Important Domain+Path Features ===\")\n",
    "    rf_combined.fit(X_train_combined, y_train)\n",
    "    # For combined features, we need to concatenate the feature names\n",
    "    feature_names_domain = domain_vectorizer.get_feature_names_out()\n",
    "    feature_names_path = path_vectorizer.get_feature_names_out()\n",
    "    feature_names_combined = np.concatenate([\n",
    "        [f\"domain_{name}\" for name in feature_names_domain],\n",
    "        [f\"path_{name}\" for name in feature_names_path]\n",
    "    ])\n",
    "    print_feature_importance(rf_combined, feature_names_combined, \"Random Forest - Domain+Path Features\")\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.sort_values('Accuracy', ascending=False, inplace=True)\n",
    "    return results_df\n",
    "\n",
    "def print_feature_importance(model, feature_names, title, top_n=20):\n",
    "    # Get feature importances\n",
    "    importances = model.feature_importances_\n",
    "    \n",
    "    # Sort feature importances in descending order\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    \n",
    "    # Select top N features\n",
    "    top_indices = indices[:top_n]\n",
    "    top_importances = importances[top_indices]\n",
    "    top_feature_names = [feature_names[i] for i in top_indices]\n",
    "    \n",
    "    # Print feature ranking\n",
    "    print(f\"\\nTop {top_n} features for {title}:\")\n",
    "    for i, idx in enumerate(top_indices):\n",
    "        if i < top_n:\n",
    "            print(f\"{i+1}. {feature_names[idx]} ({importances[idx]:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "653092c6-37c4-4905-985b-36cc7347bd13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting URL Classification with Multi-class Labels...\n",
      "Total samples: 31598\n",
      "Label distribution:\n",
      "  News: 26899 samples\n",
      "  Business & E-Commerce: 1344 samples\n",
      "  Entertainment & Culture: 1000 samples\n",
      "  Science, Academia, & Technology: 711 samples\n",
      "  General Information & Education: 627 samples\n",
      "  Social Media/Forums: 294 samples\n",
      "  Legal & Policy: 237 samples\n",
      "  Other: 192 samples\n",
      "  Blogs: 184 samples\n",
      "  Books: 110 samples\n",
      "No test set found, creating from train set...\n",
      "Training samples: 25278\n",
      "Testing samples: 6320\n",
      "\n",
      "=== Approach 1: Using Lexical Features ===\n",
      "\n",
      "Decision Tree (Lexical) Results:\n",
      "Training time: 0.1178 seconds\n",
      "Inference time: 0.0008 seconds\n",
      "Accuracy: 0.8119\n",
      "F1 Score: 0.8154\n",
      "F2 Score: 0.8132\n",
      "\n",
      "Classification Report:\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                          Blogs       0.17      0.24      0.20        37\n",
      "                          Books       0.31      0.36      0.33        22\n",
      "          Business & E-Commerce       0.26      0.29      0.27       269\n",
      "        Entertainment & Culture       0.26      0.26      0.26       200\n",
      "General Information & Education       0.21      0.21      0.21       126\n",
      "                 Legal & Policy       0.23      0.34      0.27        47\n",
      "                           News       0.92      0.91      0.91      5380\n",
      "                          Other       0.27      0.29      0.28        38\n",
      "Science, Academia, & Technology       0.33      0.36      0.34       142\n",
      "            Social Media/Forums       0.20      0.15      0.17        59\n",
      "\n",
      "                       accuracy                           0.81      6320\n",
      "                      macro avg       0.32      0.34      0.33      6320\n",
      "                   weighted avg       0.82      0.81      0.82      6320\n",
      "\n",
      "\n",
      "KNN (Lexical) Results:\n",
      "Training time: 0.0099 seconds\n",
      "Inference time: 0.1809 seconds\n",
      "Accuracy: 0.8416\n",
      "F1 Score: 0.8313\n",
      "F2 Score: 0.8370\n",
      "\n",
      "Classification Report:\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                          Blogs       0.19      0.30      0.23        37\n",
      "                          Books       0.25      0.27      0.26        22\n",
      "          Business & E-Commerce       0.28      0.29      0.29       269\n",
      "        Entertainment & Culture       0.25      0.24      0.25       200\n",
      "General Information & Education       0.35      0.19      0.25       126\n",
      "                 Legal & Policy       0.38      0.19      0.25        47\n",
      "                           News       0.91      0.95      0.93      5380\n",
      "                          Other       0.44      0.18      0.26        38\n",
      "Science, Academia, & Technology       0.50      0.22      0.30       142\n",
      "            Social Media/Forums       0.38      0.14      0.20        59\n",
      "\n",
      "                       accuracy                           0.84      6320\n",
      "                      macro avg       0.39      0.30      0.32      6320\n",
      "                   weighted avg       0.83      0.84      0.83      6320\n",
      "\n",
      "\n",
      "Random Forest (Lexical) Results:\n",
      "Training time: 0.2879 seconds\n",
      "Inference time: 0.0162 seconds\n",
      "Accuracy: 0.8685\n",
      "F1 Score: 0.8390\n",
      "F2 Score: 0.8550\n",
      "\n",
      "Classification Report:\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                          Blogs       0.43      0.16      0.24        37\n",
      "                          Books       0.83      0.23      0.36        22\n",
      "          Business & E-Commerce       0.42      0.22      0.28       269\n",
      "        Entertainment & Culture       0.52      0.18      0.27       200\n",
      "General Information & Education       0.54      0.15      0.24       126\n",
      "                 Legal & Policy       0.65      0.23      0.34        47\n",
      "                           News       0.89      0.98      0.93      5380\n",
      "                          Other       0.78      0.18      0.30        38\n",
      "Science, Academia, & Technology       0.71      0.26      0.38       142\n",
      "            Social Media/Forums       0.62      0.17      0.27        59\n",
      "\n",
      "                       accuracy                           0.87      6320\n",
      "                      macro avg       0.64      0.28      0.36      6320\n",
      "                   weighted avg       0.84      0.87      0.84      6320\n",
      "\n",
      "\n",
      "=== Most Important Lexical Features ===\n",
      "\n",
      "Top 20 features for Random Forest - Lexical Features:\n",
      "1. url_length (0.1526)\n",
      "2. path_length (0.1399)\n",
      "3. domain_length (0.1387)\n",
      "4. num_hyphens (0.0848)\n",
      "5. path_digit_ratio (0.0735)\n",
      "6. num_digits (0.0574)\n",
      "7. num_slashes (0.0508)\n",
      "8. num_path_tokens (0.0449)\n",
      "9. num_dots (0.0364)\n",
      "10. tld_length (0.0328)\n",
      "11. has_https (0.0224)\n",
      "12. has_www (0.0210)\n",
      "13. subdomain_depth (0.0208)\n",
      "14. num_underscores (0.0194)\n",
      "15. has_news_in_path (0.0138)\n",
      "16. has_news_in_domain (0.0128)\n",
      "17. query_length (0.0119)\n",
      "18. has_blog_in_url (0.0099)\n",
      "19. has_article_in_path (0.0094)\n",
      "20. domain_digit_ratio (0.0094)\n",
      "\n",
      "=== Approach 2: Using Character N-grams ===\n",
      "\n",
      "Decision Tree (Char N-grams) Results:\n",
      "Training time: 17.5857 seconds\n",
      "Inference time: 0.0061 seconds\n",
      "Accuracy: 0.8116\n",
      "F1 Score: 0.8088\n",
      "F2 Score: 0.8104\n",
      "\n",
      "Classification Report:\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                          Blogs       0.19      0.11      0.14        37\n",
      "                          Books       0.25      0.27      0.26        22\n",
      "          Business & E-Commerce       0.22      0.23      0.23       269\n",
      "        Entertainment & Culture       0.25      0.23      0.23       200\n",
      "General Information & Education       0.22      0.19      0.21       126\n",
      "                 Legal & Policy       0.22      0.23      0.22        47\n",
      "                           News       0.91      0.92      0.91      5380\n",
      "                          Other       0.16      0.16      0.16        38\n",
      "Science, Academia, & Technology       0.25      0.25      0.25       142\n",
      "            Social Media/Forums       0.18      0.15      0.17        59\n",
      "\n",
      "                       accuracy                           0.81      6320\n",
      "                      macro avg       0.28      0.27      0.28      6320\n",
      "                   weighted avg       0.81      0.81      0.81      6320\n",
      "\n",
      "\n",
      "KNN (Char N-grams) Results:\n",
      "Training time: 0.0122 seconds\n",
      "Inference time: 162.4060 seconds\n",
      "Accuracy: 0.8991\n",
      "F1 Score: 0.8916\n",
      "F2 Score: 0.8955\n",
      "\n",
      "Classification Report:\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                          Blogs       0.50      0.68      0.57        37\n",
      "                          Books       0.50      0.55      0.52        22\n",
      "          Business & E-Commerce       0.55      0.55      0.55       269\n",
      "        Entertainment & Culture       0.60      0.49      0.54       200\n",
      "General Information & Education       0.56      0.35      0.43       126\n",
      "                 Legal & Policy       0.65      0.32      0.43        47\n",
      "                           News       0.94      0.97      0.96      5380\n",
      "                          Other       0.80      0.42      0.55        38\n",
      "Science, Academia, & Technology       0.83      0.47      0.60       142\n",
      "            Social Media/Forums       0.64      0.27      0.38        59\n",
      "\n",
      "                       accuracy                           0.90      6320\n",
      "                      macro avg       0.66      0.51      0.55      6320\n",
      "                   weighted avg       0.89      0.90      0.89      6320\n",
      "\n",
      "\n",
      "Random Forest (Char N-grams) Results:\n",
      "Training time: 6.5293 seconds\n",
      "Inference time: 0.0338 seconds\n",
      "Accuracy: 0.8666\n",
      "F1 Score: 0.8213\n",
      "F2 Score: 0.8460\n",
      "\n",
      "Classification Report:\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                          Blogs       0.75      0.08      0.15        37\n",
      "                          Books       1.00      0.23      0.37        22\n",
      "          Business & E-Commerce       0.48      0.11      0.18       269\n",
      "        Entertainment & Culture       0.84      0.10      0.19       200\n",
      "General Information & Education       0.87      0.10      0.18       126\n",
      "                 Legal & Policy       1.00      0.17      0.29        47\n",
      "                           News       0.87      1.00      0.93      5380\n",
      "                          Other       1.00      0.16      0.27        38\n",
      "Science, Academia, & Technology       0.95      0.14      0.25       142\n",
      "            Social Media/Forums       1.00      0.08      0.16        59\n",
      "\n",
      "                       accuracy                           0.87      6320\n",
      "                      macro avg       0.88      0.22      0.30      6320\n",
      "                   weighted avg       0.86      0.87      0.82      6320\n",
      "\n",
      "\n",
      "=== Most Important Character N-gram Features ===\n",
      "\n",
      "Top 20 features for Random Forest - Character N-gram Features:\n",
      "1. ps: (0.0073)\n",
      "2. ht (0.0064)\n",
      "3. ps:// (0.0063)\n",
      "4. tps:// (0.0061)\n",
      "5. ttp (0.0060)\n",
      "6. ne (0.0060)\n",
      "7. om/ (0.0055)\n",
      "8. m/ (0.0053)\n",
      "9. tps: (0.0050)\n",
      "10. ps:/ (0.0049)\n",
      "11. tps:/ (0.0049)\n",
      "12. htt (0.0048)\n",
      "13. .c (0.0048)\n",
      "14. tp (0.0048)\n",
      "15. .com/ (0.0048)\n",
      "16. // (0.0047)\n",
      "17. ttps (0.0047)\n",
      "18. :// (0.0045)\n",
      "19. com (0.0045)\n",
      "20. s: (0.0045)\n",
      "\n",
      "=== Approach 3: Using Domain-Specific Features ===\n",
      "\n",
      "Decision Tree (Domain+Path) Results:\n",
      "Training time: 4.0583 seconds\n",
      "Inference time: 0.0034 seconds\n",
      "Accuracy: 0.9506\n",
      "F1 Score: 0.9488\n",
      "F2 Score: 0.9496\n",
      "\n",
      "Classification Report:\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                          Blogs       0.88      0.76      0.81        37\n",
      "                          Books       0.90      0.82      0.86        22\n",
      "          Business & E-Commerce       0.84      0.71      0.77       269\n",
      "        Entertainment & Culture       0.92      0.73      0.82       200\n",
      "General Information & Education       0.85      0.72      0.78       126\n",
      "                 Legal & Policy       0.93      0.83      0.88        47\n",
      "                           News       0.96      0.99      0.97      5380\n",
      "                          Other       1.00      0.74      0.85        38\n",
      "Science, Academia, & Technology       0.91      0.79      0.85       142\n",
      "            Social Media/Forums       0.79      0.83      0.81        59\n",
      "\n",
      "                       accuracy                           0.95      6320\n",
      "                      macro avg       0.90      0.79      0.84      6320\n",
      "                   weighted avg       0.95      0.95      0.95      6320\n",
      "\n",
      "\n",
      "KNN (Domain+Path) Results:\n",
      "Training time: 0.0103 seconds\n",
      "Inference time: 14.7887 seconds\n",
      "Accuracy: 0.8872\n",
      "F1 Score: 0.8870\n",
      "F2 Score: 0.8870\n",
      "\n",
      "Classification Report:\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                          Blogs       0.31      0.35      0.33        37\n",
      "                          Books       0.72      0.59      0.65        22\n",
      "          Business & E-Commerce       0.57      0.58      0.58       269\n",
      "        Entertainment & Culture       0.54      0.55      0.54       200\n",
      "General Information & Education       0.52      0.52      0.52       126\n",
      "                 Legal & Policy       0.64      0.49      0.55        47\n",
      "                           News       0.95      0.95      0.95      5380\n",
      "                          Other       0.65      0.53      0.58        38\n",
      "Science, Academia, & Technology       0.54      0.64      0.59       142\n",
      "            Social Media/Forums       0.55      0.36      0.43        59\n",
      "\n",
      "                       accuracy                           0.89      6320\n",
      "                      macro avg       0.60      0.56      0.57      6320\n",
      "                   weighted avg       0.89      0.89      0.89      6320\n",
      "\n",
      "\n",
      "Random Forest (Domain+Path) Results:\n",
      "Training time: 1.8512 seconds\n",
      "Inference time: 0.0291 seconds\n",
      "Accuracy: 0.9755\n",
      "F1 Score: 0.9746\n",
      "F2 Score: 0.9749\n",
      "\n",
      "Classification Report:\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                          Blogs       1.00      0.89      0.94        37\n",
      "                          Books       1.00      0.77      0.87        22\n",
      "          Business & E-Commerce       0.96      0.83      0.89       269\n",
      "        Entertainment & Culture       0.99      0.83      0.91       200\n",
      "General Information & Education       0.99      0.86      0.92       126\n",
      "                 Legal & Policy       1.00      0.91      0.96        47\n",
      "                           News       0.97      1.00      0.99      5380\n",
      "                          Other       1.00      0.79      0.88        38\n",
      "Science, Academia, & Technology       1.00      0.85      0.92       142\n",
      "            Social Media/Forums       1.00      0.83      0.91        59\n",
      "\n",
      "                       accuracy                           0.98      6320\n",
      "                      macro avg       0.99      0.86      0.92      6320\n",
      "                   weighted avg       0.98      0.98      0.97      6320\n",
      "\n",
      "\n",
      "=== Most Important Domain+Path Features ===\n",
      "\n",
      "Top 20 features for Random Forest - Domain+Path Features:\n",
      "1. domain_com (0.0147)\n",
      "2. domain_om (0.0139)\n",
      "3. domain_.co (0.0129)\n",
      "4. domain_.c (0.0126)\n",
      "5. domain_co (0.0123)\n",
      "6. domain_.com (0.0111)\n",
      "7. domain_www. (0.0091)\n",
      "8. domain_ww. (0.0089)\n",
      "9. domain_w. (0.0084)\n",
      "10. domain_www (0.0081)\n",
      "11. domain_ww (0.0081)\n",
      "12. domain_s. (0.0062)\n",
      "13. domain_ar (0.0060)\n",
      "14. domain_om. (0.0059)\n",
      "15. domain_ne (0.0048)\n",
      "16. domain_s.co (0.0047)\n",
      "17. domain_re (0.0047)\n",
      "18. domain_er (0.0046)\n",
      "19. domain_s.com (0.0046)\n",
      "20. domain_or (0.0044)\n",
      "\n",
      "=== Final Comparison ===\n",
      "                          Model  Train Time (s)  Inference Time (s)  Accuracy  \\\n",
      "8   Random Forest (Domain+Path)        1.851177            0.029116  0.975475   \n",
      "6   Decision Tree (Domain+Path)        4.058303            0.003392  0.950633   \n",
      "4            KNN (Char N-grams)        0.012188          162.406031  0.899051   \n",
      "7             KNN (Domain+Path)        0.010350           14.788654  0.887184   \n",
      "2       Random Forest (Lexical)        0.287884            0.016178  0.868513   \n",
      "5  Random Forest (Char N-grams)        6.529289            0.033837  0.866614   \n",
      "1                 KNN (Lexical)        0.009930            0.180885  0.841614   \n",
      "0       Decision Tree (Lexical)        0.117758            0.000784  0.811867   \n",
      "3  Decision Tree (Char N-grams)       17.585677            0.006134  0.811551   \n",
      "\n",
      "   F1 Score  F2 Score  \n",
      "8  0.974585  0.974904  \n",
      "6  0.948755  0.949649  \n",
      "4  0.891622  0.895526  \n",
      "7  0.887006  0.887026  \n",
      "2  0.838972  0.854986  \n",
      "5  0.821311  0.845972  \n",
      "1  0.831255  0.837046  \n",
      "0  0.815352  0.813195  \n",
      "3  0.808816  0.810431  \n"
     ]
    }
   ],
   "source": [
    "# Run benchmarks\n",
    "url_df = url_df[[\"url\",\"domain\",\"label\",\"set\"]]\n",
    "results_df = classify_urls_multilabel(url_df)\n",
    "#results_df = run_benchmarks(df)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n=== Final Comparison ===\")\n",
    "print(results_df.sort_values('Accuracy', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a5993a5f-5f75-45d3-860b-9a825708cf13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Train Time (s)</th>\n",
       "      <th>Inference Time (s)</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>F2 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Random Forest (Domain+Path)</td>\n",
       "      <td>1.851177</td>\n",
       "      <td>0.029116</td>\n",
       "      <td>0.975475</td>\n",
       "      <td>0.974585</td>\n",
       "      <td>0.974904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Decision Tree (Domain+Path)</td>\n",
       "      <td>4.058303</td>\n",
       "      <td>0.003392</td>\n",
       "      <td>0.950633</td>\n",
       "      <td>0.948755</td>\n",
       "      <td>0.949649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KNN (Char N-grams)</td>\n",
       "      <td>0.012188</td>\n",
       "      <td>162.406031</td>\n",
       "      <td>0.899051</td>\n",
       "      <td>0.891622</td>\n",
       "      <td>0.895526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>KNN (Domain+Path)</td>\n",
       "      <td>0.010350</td>\n",
       "      <td>14.788654</td>\n",
       "      <td>0.887184</td>\n",
       "      <td>0.887006</td>\n",
       "      <td>0.887026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest (Lexical)</td>\n",
       "      <td>0.287884</td>\n",
       "      <td>0.016178</td>\n",
       "      <td>0.868513</td>\n",
       "      <td>0.838972</td>\n",
       "      <td>0.854986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Random Forest (Char N-grams)</td>\n",
       "      <td>6.529289</td>\n",
       "      <td>0.033837</td>\n",
       "      <td>0.866614</td>\n",
       "      <td>0.821311</td>\n",
       "      <td>0.845972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KNN (Lexical)</td>\n",
       "      <td>0.009930</td>\n",
       "      <td>0.180885</td>\n",
       "      <td>0.841614</td>\n",
       "      <td>0.831255</td>\n",
       "      <td>0.837046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Decision Tree (Lexical)</td>\n",
       "      <td>0.117758</td>\n",
       "      <td>0.000784</td>\n",
       "      <td>0.811867</td>\n",
       "      <td>0.815352</td>\n",
       "      <td>0.813195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Decision Tree (Char N-grams)</td>\n",
       "      <td>17.585677</td>\n",
       "      <td>0.006134</td>\n",
       "      <td>0.811551</td>\n",
       "      <td>0.808816</td>\n",
       "      <td>0.810431</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Model  Train Time (s)  Inference Time (s)  Accuracy  \\\n",
       "8   Random Forest (Domain+Path)        1.851177            0.029116  0.975475   \n",
       "6   Decision Tree (Domain+Path)        4.058303            0.003392  0.950633   \n",
       "4            KNN (Char N-grams)        0.012188          162.406031  0.899051   \n",
       "7             KNN (Domain+Path)        0.010350           14.788654  0.887184   \n",
       "2       Random Forest (Lexical)        0.287884            0.016178  0.868513   \n",
       "5  Random Forest (Char N-grams)        6.529289            0.033837  0.866614   \n",
       "1                 KNN (Lexical)        0.009930            0.180885  0.841614   \n",
       "0       Decision Tree (Lexical)        0.117758            0.000784  0.811867   \n",
       "3  Decision Tree (Char N-grams)       17.585677            0.006134  0.811551   \n",
       "\n",
       "   F1 Score  F2 Score  \n",
       "8  0.974585  0.974904  \n",
       "6  0.948755  0.949649  \n",
       "4  0.891622  0.895526  \n",
       "7  0.887006  0.887026  \n",
       "2  0.838972  0.854986  \n",
       "5  0.821311  0.845972  \n",
       "1  0.831255  0.837046  \n",
       "0  0.815352  0.813195  \n",
       "3  0.808816  0.810431  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c89c8ac-ea92-4d90-8f52-06fc032bc829",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
