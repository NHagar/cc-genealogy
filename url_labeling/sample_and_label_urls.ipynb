{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "932bc267-ea7e-44fd-b649-348d057961c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>label</th>\n",
       "      <th>label_source</th>\n",
       "      <th>set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10167</th>\n",
       "      <td>tricountysentry.com</td>\n",
       "      <td>News</td>\n",
       "      <td>northeastern_domain_demo</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11414</th>\n",
       "      <td>redherring.com</td>\n",
       "      <td>News</td>\n",
       "      <td>northeastern_domain_demo</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11855</th>\n",
       "      <td>thesyrinx.com</td>\n",
       "      <td>News</td>\n",
       "      <td>northeastern_domain_demo</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7348</th>\n",
       "      <td>herald-publishing.com</td>\n",
       "      <td>News</td>\n",
       "      <td>northeastern_domain_demo</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11651</th>\n",
       "      <td>surpriseindependent.com</td>\n",
       "      <td>News</td>\n",
       "      <td>northeastern_domain_demo</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8046</th>\n",
       "      <td>laprensanwa.com</td>\n",
       "      <td>News</td>\n",
       "      <td>northeastern_domain_demo</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3443</th>\n",
       "      <td>best-car-lease-deals.co.uk</td>\n",
       "      <td>Entertainment &amp; Culture</td>\n",
       "      <td>data_provenance_init</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>benzinga.com</td>\n",
       "      <td>News</td>\n",
       "      <td>northeastern_domain_demo</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1967</th>\n",
       "      <td>assetverification.com</td>\n",
       "      <td>Business &amp; E-Commerce</td>\n",
       "      <td>data_provenance_init</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11367</th>\n",
       "      <td>patriotpost.us</td>\n",
       "      <td>News</td>\n",
       "      <td>northeastern_domain_demo</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           domain                    label  \\\n",
       "10167         tricountysentry.com                     News   \n",
       "11414              redherring.com                     News   \n",
       "11855               thesyrinx.com                     News   \n",
       "7348        herald-publishing.com                     News   \n",
       "11651     surpriseindependent.com                     News   \n",
       "8046              laprensanwa.com                     News   \n",
       "3443   best-car-lease-deals.co.uk  Entertainment & Culture   \n",
       "571                  benzinga.com                     News   \n",
       "1967        assetverification.com    Business & E-Commerce   \n",
       "11367              patriotpost.us                     News   \n",
       "\n",
       "                   label_source    set  \n",
       "10167  northeastern_domain_demo  train  \n",
       "11414  northeastern_domain_demo  train  \n",
       "11855  northeastern_domain_demo  train  \n",
       "7348   northeastern_domain_demo   test  \n",
       "11651  northeastern_domain_demo  train  \n",
       "8046   northeastern_domain_demo  train  \n",
       "3443       data_provenance_init  train  \n",
       "571    northeastern_domain_demo  train  \n",
       "1967       data_provenance_init  train  \n",
       "11367  northeastern_domain_demo  train  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download, list_repo_files\n",
    "from tqdm import tqdm\n",
    "import dask.dataframe as dd\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "labels = pd.read_csv('../data/combined_domain_labels_16k_splits.csv')\n",
    "labels.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4831d30-3a8b-43e8-9f0d-812952b500d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "datasets = {\n",
    "    #\"zyda_main\": \"hf://datasets/nhagar/zyda_urls/**/*.parquet\",\n",
    "    \"zyda_fwe3\": \"hf://datasets/nhagar/zyda-2_urls_fwe3/**/*.parquet\",\n",
    "    \"zyda_dclm_crossdeduped\": \"hf://datasets/nhagar/zyda-2_urls_dclm_crossdeduped/**/*.parquet\",\n",
    "    \"dclm_baseline_batch4\": \"hf://datasets/nhagar/dclm-baseline-1.0-parquet_urls/batch_4/train-*.parquet\",\n",
    "    \"dclm_dedup\": \"hf://datasets/nhagar/dclm-dedup_urls/**/*.parquet\",\n",
    "    \"falcon_refinedweb\": \"hf://datasets/nhagar/falcon-refinedweb_urls/batch*/train-*.parquet\",\n",
    "    \"falcon_main\": \"hf://datasets/nhagar/falcon_urls/data/train-*.parquet\",\n",
    "    \"c4_en\": \"hf://datasets/nhagar/c4_en_urls/data/train-*.parquet\",\n",
    "    \"cultura\": \"hf://datasets/nhagar/cultura_urls/data/train-*.parquet\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c42b9f9-0f74-425b-839e-3806827a75eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 parquet files for nhagar/zyda-2_urls_zyda_crossdeduped-filtered\n",
      "Downloading all parquet files...\n",
      "Downloaded 1 files\n",
      "First file path: hf_cache/datasets--nhagar--zyda-2_urls_zyda_crossdeduped-filtered/snapshots/695209cf7133a596fc999304fa623e802439281f/batch_1.parquet\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:07<00:00,  7.97s/it]\n"
     ]
    }
   ],
   "source": [
    "DATASETS = [\"nhagar/zyda-2_urls_zyda_crossdeduped-filtered\",\n",
    "            #\"nhagar/falcon_urls\"]\n",
    "           ]\n",
    "\n",
    "           \n",
    "for dataset in tqdm(DATASETS):\n",
    "    try:\n",
    "        # Get files list from repo\n",
    "        files = [f for f in list_repo_files(dataset, repo_type=\"dataset\") \n",
    "                if f.endswith('.parquet')]\n",
    "        \n",
    "        if not files:\n",
    "            print(f\"No parquet files found for {dataset}, skipping\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Found {len(files)} parquet files for {dataset}\")\n",
    "        print(f\"Downloading all parquet files...\")\n",
    "\n",
    "        downloaded_files = []\n",
    "        \n",
    "        for file in files:\n",
    "            download_path = hf_hub_download(\n",
    "                repo_id=dataset,\n",
    "                filename=file,\n",
    "                repo_type=\"dataset\",\n",
    "                cache_dir=\"hf_cache\"\n",
    "            )\n",
    "            downloaded_files.append(download_path)\n",
    "        \n",
    "        print(f\"Downloaded {len(downloaded_files)} files\")\n",
    "        print(f\"First file path: {downloaded_files[0]}\")\n",
    "        \n",
    "        # Use the actual downloaded paths directly\n",
    "        print(\"Processing...\")\n",
    "        df = dd.read_parquet(downloaded_files).compute()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error with {dataset}: {str(e)}\")\n",
    "        # Save progress on error\n",
    "        dataset_name = dataset.replace('nhagar/','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b423731-2775-4e53-a9f6-5cd1880c260b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>domain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.hennsnoxlaw.com/faqs</td>\n",
       "      <td>hennsnoxlaw.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://store.basscentral.com/dingwall/dingwal...</td>\n",
       "      <td>basscentral.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://theplayfullife.polarnopyretusa.com/name...</td>\n",
       "      <td>polarnopyretusa.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://www.katephillipsevents.com/contact</td>\n",
       "      <td>katephillipsevents.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.littleroomunderthestairs.com/2015/...</td>\n",
       "      <td>littleroomunderthestairs.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0                   https://www.hennsnoxlaw.com/faqs   \n",
       "1  https://store.basscentral.com/dingwall/dingwal...   \n",
       "2  http://theplayfullife.polarnopyretusa.com/name...   \n",
       "3          http://www.katephillipsevents.com/contact   \n",
       "4  https://www.littleroomunderthestairs.com/2015/...   \n",
       "\n",
       "                         domain  \n",
       "0               hennsnoxlaw.com  \n",
       "1               basscentral.com  \n",
       "2           polarnopyretusa.com  \n",
       "3        katephillipsevents.com  \n",
       "4  littleroomunderthestairs.com  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter to labeled domains\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c3998cf-2ced-4c93-83db-0ca36a350adb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40e56a3fdf6442969497030bd9b79105",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering domains:   0%|          | 0/1912 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def filter_with_progress(df, domain_set, batch_size=100000):\n",
    "    start_time = time.time()\n",
    "    total_rows = len(df)\n",
    "    filtered_rows = []\n",
    "    \n",
    "    for i in tqdm(range(0, total_rows, batch_size), desc=\"Filtering domains\"):\n",
    "        batch = df.iloc[i:min(i+batch_size, total_rows)]\n",
    "        filtered_batch = batch[batch['domain'].isin(domain_set)]\n",
    "        filtered_rows.append(filtered_batch)\n",
    "        \n",
    "        # Show additional progress info\n",
    "        if (i + batch_size) % (batch_size * 10) == 0 or (i + batch_size) >= total_rows:\n",
    "            elapsed = time.time() - start_time\n",
    "            #print(f\"Processed {min(i+batch_size, total_rows)}/{total_rows} rows ({(min(i+batch_size, total_rows)/total_rows)*100:.1f}%) in {elapsed:.1f}s\")\n",
    "    \n",
    "    return pd.concat(filtered_rows, ignore_index=True)\n",
    "\n",
    "# Use the function\n",
    "domain_set = set(labels[labels.set=='train']['domain'])\n",
    "filtered_df = filter_with_progress(df, domain_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e7957c8-0707-48df-8d23-fcfcf44da5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 3 urls from each domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e8da915c-5372-4774-8e79-02d3a886565b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000/14256 domains\n",
      "Processed 2000/14256 domains\n",
      "Processed 3000/14256 domains\n",
      "Processed 4000/14256 domains\n",
      "Processed 5000/14256 domains\n",
      "Processed 6000/14256 domains\n",
      "Processed 7000/14256 domains\n",
      "Processed 8000/14256 domains\n",
      "Processed 9000/14256 domains\n",
      "Processed 10000/14256 domains\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "result_data = []\n",
    "processed_domains = set()\n",
    "\n",
    "# Group and process in one pass\n",
    "for domain, group in filtered_df.groupby('domain'):\n",
    "    if domain not in domain_set:\n",
    "        continue\n",
    "        \n",
    "    urls = group['url'].head(3).tolist()\n",
    "    result_data.extend([(domain, url) for url in urls])\n",
    "    processed_domains.add(domain)\n",
    "    \n",
    "    if len(processed_domains) % 1000 == 0:\n",
    "        print(f\"Processed {len(processed_domains)}/{len(domain_set)} domains\")\n",
    "    \n",
    "    # Stop if we have all domains\n",
    "    if len(processed_domains) == len(domain_set):\n",
    "        break\n",
    "\n",
    "url_df = pd.DataFrame(result_data, columns=['domain', 'url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8b86f850-b886-42a2-87a5-7da38b4b359a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>url</th>\n",
       "      <th>label</th>\n",
       "      <th>label_source</th>\n",
       "      <th>set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10594</th>\n",
       "      <td>heal.com</td>\n",
       "      <td>https://heal.com/healthrecords/</td>\n",
       "      <td>Science, Academia, &amp; Technology</td>\n",
       "      <td>data_provenance_init</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13068</th>\n",
       "      <td>kiowacountypress.net</td>\n",
       "      <td>https://kiowacountypress.net/content/cdot-hold...</td>\n",
       "      <td>News</td>\n",
       "      <td>northeastern_domain_demo</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8378</th>\n",
       "      <td>fbnewsleader.com</td>\n",
       "      <td>https://www.fbnewsleader.com/news/coyote-sight...</td>\n",
       "      <td>News</td>\n",
       "      <td>northeastern_domain_demo</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29809</th>\n",
       "      <td>westword.com</td>\n",
       "      <td>https://www.westword.com/best-of/2019/shopping...</td>\n",
       "      <td>News</td>\n",
       "      <td>northeastern_domain_demo</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24644</th>\n",
       "      <td>tecumsehchieftain.com</td>\n",
       "      <td>https://www.tecumsehchieftain.com/articles/new...</td>\n",
       "      <td>News</td>\n",
       "      <td>northeastern_domain_demo</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22272</th>\n",
       "      <td>sanpedronewspilot.com</td>\n",
       "      <td>http://sanpedronewspilot.com/profiles/blogs/20...</td>\n",
       "      <td>News</td>\n",
       "      <td>northeastern_domain_demo</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30142</th>\n",
       "      <td>wicd15.com</td>\n",
       "      <td>http://wicd15.com/template/cgi-bin/archived.pl...</td>\n",
       "      <td>News</td>\n",
       "      <td>northeastern_domain_demo</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9408</th>\n",
       "      <td>gasconadecountyrepublican.com</td>\n",
       "      <td>http://gasconadecountyrepublican.com/content/g...</td>\n",
       "      <td>News</td>\n",
       "      <td>northeastern_domain_demo</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6145</th>\n",
       "      <td>dailynexus.com</td>\n",
       "      <td>http://dailynexus.com/2019-04-12/rowan-blasts-...</td>\n",
       "      <td>News</td>\n",
       "      <td>northeastern_domain_demo</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26116</th>\n",
       "      <td>themonitor.net</td>\n",
       "      <td>https://www.themonitor.net/article/traffic-sto...</td>\n",
       "      <td>News</td>\n",
       "      <td>northeastern_domain_demo</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              domain  \\\n",
       "10594                       heal.com   \n",
       "13068           kiowacountypress.net   \n",
       "8378                fbnewsleader.com   \n",
       "29809                   westword.com   \n",
       "24644          tecumsehchieftain.com   \n",
       "22272          sanpedronewspilot.com   \n",
       "30142                     wicd15.com   \n",
       "9408   gasconadecountyrepublican.com   \n",
       "6145                  dailynexus.com   \n",
       "26116                 themonitor.net   \n",
       "\n",
       "                                                     url  \\\n",
       "10594                    https://heal.com/healthrecords/   \n",
       "13068  https://kiowacountypress.net/content/cdot-hold...   \n",
       "8378   https://www.fbnewsleader.com/news/coyote-sight...   \n",
       "29809  https://www.westword.com/best-of/2019/shopping...   \n",
       "24644  https://www.tecumsehchieftain.com/articles/new...   \n",
       "22272  http://sanpedronewspilot.com/profiles/blogs/20...   \n",
       "30142  http://wicd15.com/template/cgi-bin/archived.pl...   \n",
       "9408   http://gasconadecountyrepublican.com/content/g...   \n",
       "6145   http://dailynexus.com/2019-04-12/rowan-blasts-...   \n",
       "26116  https://www.themonitor.net/article/traffic-sto...   \n",
       "\n",
       "                                 label              label_source    set  \n",
       "10594  Science, Academia, & Technology      data_provenance_init  train  \n",
       "13068                             News  northeastern_domain_demo  train  \n",
       "8378                              News  northeastern_domain_demo  train  \n",
       "29809                             News  northeastern_domain_demo  train  \n",
       "24644                             News  northeastern_domain_demo  train  \n",
       "22272                             News  northeastern_domain_demo  train  \n",
       "30142                             News  northeastern_domain_demo  train  \n",
       "9408                              News  northeastern_domain_demo  train  \n",
       "6145                              News  northeastern_domain_demo  train  \n",
       "26116                             News  northeastern_domain_demo  train  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract features\n",
    "url_df=url_df.merge(right=labels, left_on=\"domain\", right_on=\"domain\")\n",
    "url_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e90854-e92d-4de1-bed5-b8b3b200865a",
   "metadata": {},
   "source": [
    "# Classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "eaaa1e2a-6900-49c6-b05a-b7c1f722a80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "from urllib.parse import urlparse\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, fbeta_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from openai import OpenAI\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Function to extract features from URLs\n",
    "def extract_url_features(url):\n",
    "    # Parse URL\n",
    "    parsed = urlparse(url)\n",
    "    domain = parsed.netloc\n",
    "    path = parsed.path\n",
    "    query = parsed.query\n",
    "    \n",
    "    # Extract basic features\n",
    "    features = {\n",
    "        'url_length': len(url),\n",
    "        'domain_length': len(domain),\n",
    "        'path_length': len(path),\n",
    "        'num_slashes': url.count('/'),\n",
    "        'num_dots': url.count('.'),\n",
    "        'num_equal': url.count('='),\n",
    "        'num_params': len(query.split('&')) if query else 0,\n",
    "        'has_https': int(url.startswith('https')),\n",
    "        'has_www': int('www.' in domain),\n",
    "        'num_digits': sum(c.isdigit() for c in url),\n",
    "        'num_path_tokens': len([p for p in path.split('/') if p]),\n",
    "        'has_news_in_domain': int('news' in domain.lower()),\n",
    "        'has_news_in_path': int('news' in path.lower()),\n",
    "        'has_article_in_path': int('article' in path.lower()),\n",
    "        'has_content_in_path': int('content' in path.lower()),\n",
    "        'has_story_in_path': int('story' in path.lower()),\n",
    "        'has_date_pattern': int(bool(re.search(r'/(19|20)\\d{2}[-/](0[1-9]|1[0-2])[-/](0[1-9]|[12][0-9]|3[01])/', url))),\n",
    "        'has_blog_in_url': int('blog' in url.lower()),\n",
    "    }\n",
    "    \n",
    "    return pd.Series(features)\n",
    "\n",
    "# Function to evaluate a model\n",
    "def evaluate_model(name, model, X_train, X_test, y_train, y_test):\n",
    "    # Training\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # Prediction\n",
    "    start_time = time.time()\n",
    "    y_pred = model.predict(X_test)\n",
    "    inference_time = time.time() - start_time\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    f2 = fbeta_score(y_test, y_pred, beta=2, average='weighted')\n",
    "    \n",
    "    print(f\"\\n{name} Results:\")\n",
    "    print(f\"Training time: {train_time:.4f} seconds\")\n",
    "    print(f\"Inference time: {inference_time:.4f} seconds\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"F2 Score: {f2:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    return {\n",
    "        'Model': name,\n",
    "        'Train Time (s)': train_time,\n",
    "        'Inference Time (s)': inference_time,\n",
    "        'Accuracy': accuracy,\n",
    "        'F1 Score': f1,\n",
    "        'F2 Score': f2\n",
    "    }\n",
    "\n",
    "\n",
    "def classify_url_with_llm(url, model, prompt):\n",
    "    llm = OpenAI(base_url=\"http://127.0.0.1:1234/v1\", api_key=\"lm-studio\")\n",
    "    \n",
    "    resp = llm.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": prompt},\n",
    "            {\"role\": \"user\", \"content\": url},\n",
    "        ],\n",
    "    )\n",
    "    txt = resp.choices[0].message.content\n",
    "\n",
    "    # Extract JSON output from the response\n",
    "    json_extract_pattern = re.compile(r\"```json\\n(.*?)\\n```\", re.DOTALL)\n",
    "    json_extract = json_extract_pattern.search(txt).group(1)\n",
    "\n",
    "    return json.loads(json_extract)\n",
    "\n",
    "\n",
    "\n",
    "def classify_urls(url_df):\n",
    "    print(\"Starting URL Classification for News Detection...\")\n",
    "    \n",
    "    # Convert multi-class labels to binary (news or not_news)\n",
    "    url_df[\"binary_label\"] = url_df[\"label\"].apply(lambda x: \"news\" if x == \"News\" else \"not_news\")\n",
    "    \n",
    "    # Check data\n",
    "    print(f\"Total samples: {len(url_df)}\")\n",
    "    print(f\"News samples: {sum(url_df['binary_label'] == 'news')}\")\n",
    "    print(f\"Non-news samples: {sum(url_df['binary_label'] == 'not_news')}\")\n",
    "    \n",
    "    # Split data if no test set defined\n",
    "    if 'set' not in url_df.columns or url_df['set'].isna().any():\n",
    "        print(\"Creating train/test split...\")\n",
    "        train_df, test_df = train_test_split(url_df, test_size=0.2, stratify=url_df['binary_label'], random_state=42)\n",
    "    else:\n",
    "        train_df = url_df[url_df['set'] == 'train']\n",
    "        test_df = url_df[url_df['set'] == 'test']\n",
    "        if len(test_df) == 0:  # If no test set exists\n",
    "            print(\"No test set found, creating from train set...\")\n",
    "            train_df, test_df = train_test_split(train_df, test_size=0.2, stratify=train_df['binary_label'], random_state=42)\n",
    "    \n",
    "    print(f\"Training samples: {len(train_df)}\")\n",
    "    print(f\"Testing samples: {len(test_df)}\")\n",
    "    \n",
    "    # Prepare labels\n",
    "    y_train = train_df['binary_label']\n",
    "    y_test = test_df['binary_label']\n",
    "    \n",
    "    # Store results\n",
    "    results = []\n",
    "    \n",
    "    # Approach 1: Lexical Features\n",
    "    print(\"\\n=== Approach 1: Using Lexical Features ===\")\n",
    "    X_train_lex = train_df['url'].apply(extract_url_features)\n",
    "    X_test_lex = test_df['url'].apply(extract_url_features)\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_lex_scaled = scaler.fit_transform(X_train_lex)\n",
    "    X_test_lex_scaled = scaler.transform(X_test_lex)\n",
    "    \n",
    "    # Models\n",
    "    dt_lex = DecisionTreeClassifier(max_depth=10, random_state=42)\n",
    "    knn_lex = KNeighborsClassifier(n_neighbors=5)\n",
    "    rf_lex = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    \n",
    "    # Evaluate\n",
    "    results.append(evaluate_model(\"Decision Tree (Lexical)\", dt_lex, X_train_lex_scaled, X_test_lex_scaled, y_train, y_test))\n",
    "    results.append(evaluate_model(\"KNN (Lexical)\", knn_lex, X_train_lex_scaled, X_test_lex_scaled, y_train, y_test))\n",
    "    results.append(evaluate_model(\"Random Forest (Lexical)\", rf_lex, X_train_lex_scaled, X_test_lex_scaled, y_train, y_test))\n",
    "\n",
    "    \n",
    "    # Approach 2: Character N-grams\n",
    "    print(\"\\n=== Approach 2: Using Character N-grams ===\")\n",
    "    char_vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(3, 5), max_features=1000)\n",
    "    X_train_char = char_vectorizer.fit_transform(train_df['url'])\n",
    "    X_test_char = char_vectorizer.transform(test_df['url'])\n",
    "    \n",
    "    # Models\n",
    "    dt_char = DecisionTreeClassifier(max_depth=20, random_state=42)\n",
    "    knn_char = KNeighborsClassifier(n_neighbors=5)\n",
    "    rf_char = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    \n",
    "    # Evaluate\n",
    "    results.append(evaluate_model(\"Decision Tree (Char N-grams)\", dt_char, X_train_char, X_test_char, y_train, y_test))\n",
    "    results.append(evaluate_model(\"KNN (Char N-grams)\", knn_char, X_train_char, X_test_char, y_train, y_test))\n",
    "    results.append(evaluate_model(\"Random Forest (Char N-grams)\", rf_char, X_train_char, X_test_char, y_train, y_test))\n",
    "    \n",
    "    # Approach 3: Domain Features\n",
    "    print(\"\\n=== Approach 3: Using Domain-Specific Features ===\")\n",
    "    # Extract URL parts\n",
    "    train_df['domain'] = train_df['url'].apply(lambda x: urlparse(x).netloc)\n",
    "    test_df['domain'] = test_df['url'].apply(lambda x: urlparse(x).netloc)\n",
    "    train_df['path'] = train_df['url'].apply(lambda x: urlparse(x).path)\n",
    "    test_df['path'] = test_df['url'].apply(lambda x: urlparse(x).path)\n",
    "    \n",
    "    # Create TF-IDF features for domains\n",
    "    domain_vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(2, 5), max_features=500)\n",
    "    X_train_domain = domain_vectorizer.fit_transform(train_df['domain'])\n",
    "    X_test_domain = domain_vectorizer.transform(test_df['domain'])\n",
    "    \n",
    "    # Create TF-IDF features for paths\n",
    "    path_vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1, 5), max_features=500, token_pattern=r'[a-zA-Z0-9]+')\n",
    "    X_train_path = path_vectorizer.fit_transform(train_df['path'])\n",
    "    X_test_path = path_vectorizer.transform(test_df['path'])\n",
    "    \n",
    "    # Combine sparse matrices\n",
    "    from scipy.sparse import hstack\n",
    "    X_train_combined = hstack([X_train_domain, X_train_path])\n",
    "    X_test_combined = hstack([X_test_domain, X_test_path])\n",
    "    \n",
    "    # Models\n",
    "    dt_combined = DecisionTreeClassifier(max_depth=100, random_state=42)\n",
    "    knn_combined = KNeighborsClassifier(n_neighbors=5)\n",
    "    rf_combined = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    \n",
    "    # Evaluate\n",
    "    results.append(evaluate_model(\"Decision Tree (Domain+Path)\", dt_combined, X_train_combined, X_test_combined, y_train, y_test))\n",
    "    results.append(evaluate_model(\"KNN (Domain+Path)\", knn_combined, X_train_combined, X_test_combined, y_train, y_test))\n",
    "    results.append(evaluate_model(\"Random Forest (Domain+Path)\", rf_combined, X_train_combined, X_test_combined, y_train, y_test))\n",
    "    \n",
    "    # Approach 4: Using LLM Classification\n",
    "    print(\"\\n=== Approach 4: Using LLM Classification ===\")\n",
    "    \n",
    "    # Load prompts\n",
    "    with open('prompt_is_news.txt', 'r') as file:\n",
    "        prompt_binary_label = file.read()\n",
    "\n",
    "    \n",
    "    # Sample a small subset of test data for LLM evaluation (LLMs are slower)\n",
    "    llm_test_size = min(1000, len(test_df))\n",
    "    llm_test_df = test_df.sample(llm_test_size, random_state=42)\n",
    "    \n",
    "    # List of models to evaluate\n",
    "    llm_models = [\n",
    "        \"gemma-2-9b-it-GGUF\",\n",
    "    ]\n",
    "    \n",
    "    for model in llm_models:\n",
    "        print(f\"\\nProcessing LLM model: {model}\")\n",
    "        \n",
    "        # Time the binary classification\n",
    "        print(f\"Starting binary classification for {model}...\")\n",
    "        start_time = time.time()\n",
    "        llm_test_df[f\"{model}_binary_label\"] = llm_test_df[\"url\"].apply(\n",
    "            classify_url_with_llm, model=model, prompt=prompt_binary_label\n",
    "        )\n",
    "        binary_time = time.time() - start_time\n",
    "        print(f\"  Completed binary classification in {binary_time:.2f}s ({binary_time/len(llm_test_df):.4f}s per URL)\")\n",
    "        \n",
    "        # Extract binary predictions\n",
    "        llm_test_df[f\"{model}_is_news\"] = llm_test_df[f\"{model}_binary_label\"].apply(\n",
    "            lambda x: x[\"is_news\"]\n",
    "        )\n",
    "        \n",
    "        # Convert to correct format for evaluation (0/1 -> news/not_news)\n",
    "        llm_test_df[f\"{model}_pred\"] = llm_test_df[f\"{model}_is_news\"].apply(\n",
    "            lambda x: \"news\" if x == 1 else \"not_news\"\n",
    "        )\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(llm_test_df[\"binary_label\"], llm_test_df[f\"{model}_pred\"])\n",
    "        f1 = f1_score(llm_test_df[\"binary_label\"], llm_test_df[f\"{model}_pred\"], pos_label=\"news\", average=\"binary\")\n",
    "        f2 = fbeta_score(llm_test_df[\"binary_label\"], llm_test_df[f\"{model}_pred\"], beta=2, pos_label=\"news\", average=\"binary\")\n",
    "        \n",
    "        # Add to results\n",
    "        results.append({\n",
    "            'Model': f\"{model} (LLM)\",\n",
    "            'Train Time (s)': binary_time,  # Total time spent on inference\n",
    "            'Inference Time (s)': binary_time / len(llm_test_df),  # Average time per URL\n",
    "            'Accuracy': accuracy,\n",
    "            'F1 Score': f1,\n",
    "            'F2 Score': f2\n",
    "        })\n",
    "        \n",
    "        print(f\"\\n{model} (LLM) Results:\")\n",
    "        print(f\"Processing time: {binary_time:.4f} seconds\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"F1 Score: {f1:.4f}\")\n",
    "        print(f\"F2 Score: {f2:.4f}\")\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(llm_test_df[\"binary_label\"], llm_test_df[f\"{model}_pred\"]))\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n=== Summary of Results ===\")\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(results_df.sort_values('Accuracy', ascending=False))\n",
    "    \n",
    "    # Feature importance for the best model (assuming it's a tree-based model)\n",
    "    best_model_name = results_df.iloc[0]['Model']\n",
    "    if 'Random Forest' in best_model_name and 'LLM' not in best_model_name:\n",
    "        if 'Lexical' in best_model_name:\n",
    "            print(\"\\nTop Feature Importances (Random Forest with Lexical Features):\")\n",
    "            feature_importances = pd.DataFrame({\n",
    "                'Feature': X_train_lex.columns,\n",
    "                'Importance': rf_lex.feature_importances_\n",
    "            }).sort_values('Importance', ascending=False)\n",
    "            print(feature_importances.head(10))\n",
    "        elif 'Domain+Path' in best_model_name:\n",
    "            print(\"\\nFeature importance analysis not available for TF-IDF features\")\n",
    "    \n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b9fda9-4db5-4c29-8884-c4a3f7077289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting URL Classification for News Detection...\n",
      "Total samples: 31598\n",
      "News samples: 26899\n",
      "Non-news samples: 4699\n",
      "No test set found, creating from train set...\n",
      "Training samples: 25278\n",
      "Testing samples: 6320\n",
      "\n",
      "=== Approach 1: Using Lexical Features ===\n",
      "\n",
      "Decision Tree (Lexical) Results:\n",
      "Training time: 0.0485 seconds\n",
      "Inference time: 0.0004 seconds\n",
      "Accuracy: 0.8634\n",
      "F1 Score: 0.8377\n",
      "F2 Score: 0.8514\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        news       0.88      0.97      0.92      5380\n",
      "    not_news       0.60      0.24      0.34       940\n",
      "\n",
      "    accuracy                           0.86      6320\n",
      "   macro avg       0.74      0.61      0.63      6320\n",
      "weighted avg       0.84      0.86      0.84      6320\n",
      "\n",
      "\n",
      "KNN (Lexical) Results:\n",
      "Training time: 0.0092 seconds\n",
      "Inference time: 0.1542 seconds\n",
      "Accuracy: 0.8646\n",
      "F1 Score: 0.8542\n",
      "F2 Score: 0.8599\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        news       0.90      0.95      0.92      5380\n",
      "    not_news       0.56      0.39      0.46       940\n",
      "\n",
      "    accuracy                           0.86      6320\n",
      "   macro avg       0.73      0.67      0.69      6320\n",
      "weighted avg       0.85      0.86      0.85      6320\n",
      "\n",
      "\n",
      "Random Forest (Lexical) Results:\n",
      "Training time: 1.2282 seconds\n",
      "Inference time: 0.0672 seconds\n",
      "Accuracy: 0.8748\n",
      "F1 Score: 0.8617\n",
      "F2 Score: 0.8687\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        news       0.90      0.96      0.93      5380\n",
      "    not_news       0.63      0.38      0.48       940\n",
      "\n",
      "    accuracy                           0.87      6320\n",
      "   macro avg       0.76      0.67      0.70      6320\n",
      "weighted avg       0.86      0.87      0.86      6320\n",
      "\n",
      "\n",
      "=== Approach 2: Using Character N-grams ===\n",
      "\n",
      "Decision Tree (Char N-grams) Results:\n",
      "Training time: 3.1791 seconds\n",
      "Inference time: 0.0039 seconds\n",
      "Accuracy: 0.8563\n",
      "F1 Score: 0.8423\n",
      "F2 Score: 0.8500\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        news       0.89      0.95      0.92      5380\n",
      "    not_news       0.53      0.33      0.41       940\n",
      "\n",
      "    accuracy                           0.86      6320\n",
      "   macro avg       0.71      0.64      0.66      6320\n",
      "weighted avg       0.84      0.86      0.84      6320\n",
      "\n",
      "\n",
      "KNN (Char N-grams) Results:\n",
      "Training time: 0.0103 seconds\n",
      "Inference time: 55.1608 seconds\n",
      "Accuracy: 0.8856\n",
      "F1 Score: 0.8843\n",
      "F2 Score: 0.8851\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        news       0.93      0.94      0.93      5380\n",
      "    not_news       0.62      0.59      0.60       940\n",
      "\n",
      "    accuracy                           0.89      6320\n",
      "   macro avg       0.78      0.76      0.77      6320\n",
      "weighted avg       0.88      0.89      0.88      6320\n",
      "\n",
      "\n",
      "Random Forest (Char N-grams) Results:\n",
      "Training time: 16.6117 seconds\n",
      "Inference time: 0.1066 seconds\n",
      "Accuracy: 0.8870\n",
      "F1 Score: 0.8709\n",
      "F2 Score: 0.8790\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        news       0.90      0.98      0.94      5380\n",
      "    not_news       0.74      0.37      0.50       940\n",
      "\n",
      "    accuracy                           0.89      6320\n",
      "   macro avg       0.82      0.68      0.72      6320\n",
      "weighted avg       0.88      0.89      0.87      6320\n",
      "\n",
      "\n",
      "=== Approach 3: Using Domain-Specific Features ===\n",
      "\n",
      "Decision Tree (Domain+Path) Results:\n",
      "Training time: 2.7612 seconds\n",
      "Inference time: 0.0034 seconds\n",
      "Accuracy: 0.9468\n",
      "F1 Score: 0.9472\n",
      "F2 Score: 0.9470\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        news       0.97      0.97      0.97      5380\n",
      "    not_news       0.81      0.84      0.82       940\n",
      "\n",
      "    accuracy                           0.95      6320\n",
      "   macro avg       0.89      0.90      0.90      6320\n",
      "weighted avg       0.95      0.95      0.95      6320\n",
      "\n",
      "\n",
      "KNN (Domain+Path) Results:\n",
      "Training time: 0.0099 seconds\n",
      "Inference time: 12.6813 seconds\n",
      "Accuracy: 0.8696\n",
      "F1 Score: 0.8726\n",
      "F2 Score: 0.8707\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        news       0.93      0.91      0.92      5380\n",
      "    not_news       0.56      0.62      0.59       940\n",
      "\n",
      "    accuracy                           0.87      6320\n",
      "   macro avg       0.74      0.77      0.75      6320\n",
      "weighted avg       0.88      0.87      0.87      6320\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run benchmarks\n",
    "url_df = url_df[[\"url\",\"domain\",\"label\",\"set\"]]\n",
    "url_df[\"binary_label\"] = url_df[\"label\"].apply(lambda x: \"news\" if x == \"News\" else \"not_news\")\n",
    "results_df = classify_urls(url_df)\n",
    "#results_df = run_benchmarks(df)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n=== Final Comparison ===\")\n",
    "print(results_df.sort_values('Accuracy', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "beed02ac-ae79-4574-bedd-7724479022e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Train Time (s)</th>\n",
       "      <th>Inference Time (s)</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>F2 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Decision Tree (Lexical)</td>\n",
       "      <td>0.048373</td>\n",
       "      <td>0.000408</td>\n",
       "      <td>0.863449</td>\n",
       "      <td>0.837657</td>\n",
       "      <td>0.851373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KNN (Lexical)</td>\n",
       "      <td>0.009825</td>\n",
       "      <td>0.149413</td>\n",
       "      <td>0.864557</td>\n",
       "      <td>0.854160</td>\n",
       "      <td>0.859859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest (Lexical)</td>\n",
       "      <td>1.283957</td>\n",
       "      <td>0.070917</td>\n",
       "      <td>0.874842</td>\n",
       "      <td>0.861726</td>\n",
       "      <td>0.868674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Decision Tree (Char N-grams)</td>\n",
       "      <td>3.458982</td>\n",
       "      <td>0.003407</td>\n",
       "      <td>0.856329</td>\n",
       "      <td>0.842262</td>\n",
       "      <td>0.849970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KNN (Char N-grams)</td>\n",
       "      <td>0.011575</td>\n",
       "      <td>56.760021</td>\n",
       "      <td>0.885601</td>\n",
       "      <td>0.884276</td>\n",
       "      <td>0.885051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Random Forest (Char N-grams)</td>\n",
       "      <td>17.281515</td>\n",
       "      <td>0.106967</td>\n",
       "      <td>0.887025</td>\n",
       "      <td>0.870851</td>\n",
       "      <td>0.879010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Decision Tree (Domain+Path)</td>\n",
       "      <td>3.053497</td>\n",
       "      <td>0.003359</td>\n",
       "      <td>0.946994</td>\n",
       "      <td>0.947235</td>\n",
       "      <td>0.947086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>KNN (Domain+Path)</td>\n",
       "      <td>0.010520</td>\n",
       "      <td>11.938275</td>\n",
       "      <td>0.880380</td>\n",
       "      <td>0.881551</td>\n",
       "      <td>0.880832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Random Forest (Domain+Path)</td>\n",
       "      <td>7.900726</td>\n",
       "      <td>0.101993</td>\n",
       "      <td>0.972468</td>\n",
       "      <td>0.971523</td>\n",
       "      <td>0.971866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>gemma-2-9b-it-GGUF (LLM)</td>\n",
       "      <td>648.644349</td>\n",
       "      <td>6.486443</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>0.898734</td>\n",
       "      <td>0.859564</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Model  Train Time (s)  Inference Time (s)  Accuracy  \\\n",
       "0       Decision Tree (Lexical)        0.048373            0.000408  0.863449   \n",
       "1                 KNN (Lexical)        0.009825            0.149413  0.864557   \n",
       "2       Random Forest (Lexical)        1.283957            0.070917  0.874842   \n",
       "3  Decision Tree (Char N-grams)        3.458982            0.003407  0.856329   \n",
       "4            KNN (Char N-grams)        0.011575           56.760021  0.885601   \n",
       "5  Random Forest (Char N-grams)       17.281515            0.106967  0.887025   \n",
       "6   Decision Tree (Domain+Path)        3.053497            0.003359  0.946994   \n",
       "7             KNN (Domain+Path)        0.010520           11.938275  0.880380   \n",
       "8   Random Forest (Domain+Path)        7.900726            0.101993  0.972468   \n",
       "9      gemma-2-9b-it-GGUF (LLM)      648.644349            6.486443  0.840000   \n",
       "\n",
       "   F1 Score  F2 Score  \n",
       "0  0.837657  0.851373  \n",
       "1  0.854160  0.859859  \n",
       "2  0.861726  0.868674  \n",
       "3  0.842262  0.849970  \n",
       "4  0.884276  0.885051  \n",
       "5  0.870851  0.879010  \n",
       "6  0.947235  0.947086  \n",
       "7  0.881551  0.880832  \n",
       "8  0.971523  0.971866  \n",
       "9  0.898734  0.859564  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b8f984-b684-4d8b-8505-0285a7841d07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
