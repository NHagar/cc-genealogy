{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef6dafff-f875-46e7-984f-f0219c592393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>label</th>\n",
       "      <th>label_source</th>\n",
       "      <th>set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14101</th>\n",
       "      <td>thecoastnews.com</td>\n",
       "      <td>Local News</td>\n",
       "      <td>northeastern_domain_demo</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8385</th>\n",
       "      <td>miamiherald.com</td>\n",
       "      <td>Local News</td>\n",
       "      <td>northeastern_domain_demo</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5838</th>\n",
       "      <td>fairfaxconnection.com</td>\n",
       "      <td>Local News</td>\n",
       "      <td>northeastern_domain_demo</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10096</th>\n",
       "      <td>redeyechicago.com</td>\n",
       "      <td>Local News</td>\n",
       "      <td>northeastern_domain_demo</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12339</th>\n",
       "      <td>unionnews-exchange.com</td>\n",
       "      <td>Local News</td>\n",
       "      <td>northeastern_domain_demo</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       domain       label              label_source    set\n",
       "14101        thecoastnews.com  Local News  northeastern_domain_demo    val\n",
       "8385          miamiherald.com  Local News  northeastern_domain_demo  train\n",
       "5838    fairfaxconnection.com  Local News  northeastern_domain_demo  train\n",
       "10096       redeyechicago.com  Local News  northeastern_domain_demo  train\n",
       "12339  unionnews-exchange.com  Local News  northeastern_domain_demo  train"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "\n",
    "from huggingface_hub import hf_hub_download, list_repo_files\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, fbeta_score, classification_report\n",
    "\n",
    "# Modeling libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "DATASETS = [\"nhagar/zyda-2_urls_zyda_crossdeduped-filtered\",\n",
    "           ]\n",
    "\n",
    "# read in ground truth labels\n",
    "ground_truth_labels = pd.read_csv('../data/combined_domain_labels_16k_splits.csv')\n",
    "ground_truth_labels.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41551d69-f429-4c5c-941a-f5ad0395fc78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 parquet files for nhagar/zyda-2_urls_zyda_crossdeduped-filtered\n",
      "Downloading all parquet files...\n",
      "Downloaded 1 files\n",
      "First file path: hf_cache/datasets--nhagar--zyda-2_urls_zyda_crossdeduped-filtered/snapshots/695209cf7133a596fc999304fa623e802439281f/batch_1.parquet\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:07<00:00,  7.20s/it]\n"
     ]
    }
   ],
   "source": [
    "for dataset in tqdm(DATASETS):\n",
    "    try:\n",
    "        # Get files list from repo\n",
    "        files = [f for f in list_repo_files(dataset, repo_type=\"dataset\") \n",
    "                if f.endswith('.parquet')]\n",
    "        if not files:\n",
    "            print(f\"No parquet files found for {dataset}, skipping\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Found {len(files)} parquet files for {dataset}\")\n",
    "        print(f\"Downloading all parquet files...\")\n",
    "        downloaded_files = []\n",
    "        \n",
    "        for file in files:\n",
    "            download_path = hf_hub_download(\n",
    "                repo_id=dataset,\n",
    "                filename=file,\n",
    "                repo_type=\"dataset\",\n",
    "                cache_dir=\"hf_cache\"\n",
    "            )\n",
    "            downloaded_files.append(download_path)\n",
    "        \n",
    "        print(f\"Downloaded {len(downloaded_files)} files\")\n",
    "        print(f\"First file path: {downloaded_files[0]}\")\n",
    "        print(\"Processing...\")\n",
    "        df = dd.read_parquet(downloaded_files).compute()\n",
    "    except Exception as e:\n",
    "        print(f\"Error with {dataset}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "857b0e43-e958-4a3b-a188-7d40e459b034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter function to get URLs from domains with a ground truth label\n",
    "\n",
    "def filter_with_progress(df, domain_set, batch_size=100000):\n",
    "    start_time = time.time()\n",
    "    total_rows = len(df)\n",
    "    filtered_rows = []\n",
    "    \n",
    "    for i in tqdm(range(0, total_rows, batch_size), desc=\"Filtering domains\"):\n",
    "        batch = df.iloc[i:min(i+batch_size, total_rows)]\n",
    "        filtered_batch = batch[batch['domain'].isin(domain_set)]\n",
    "        filtered_rows.append(filtered_batch)\n",
    "        \n",
    "        # Show additional progress info\n",
    "        if (i + batch_size) % (batch_size * 10) == 0 or (i + batch_size) >= total_rows:\n",
    "            elapsed = time.time() - start_time\n",
    "            #print(f\"Processed {min(i+batch_size, total_rows)}/{total_rows} rows ({(min(i+batch_size, total_rows)/total_rows)*100:.1f}%) in {elapsed:.1f}s\")\n",
    "    \n",
    "    return pd.concat(filtered_rows, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a8ebd25-da6a-4581-bee9-7fc9ad430c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering domains: 100%|████████████████████| 1912/1912 [01:37<00:00, 19.70it/s]\n"
     ]
    }
   ],
   "source": [
    "domain_set = set(ground_truth_labels['domain'])\n",
    "filtered_df = filter_with_progress(df, domain_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa8153e3-ed30-49b0-857a-733c387bc662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42189780"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b14afe52-c728-4431-91a7-a2eac87d9431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>domain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21147334</th>\n",
       "      <td>http://www.hindustantimes.com/india/wife-of-ma...</td>\n",
       "      <td>hindustantimes.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15195719</th>\n",
       "      <td>http://www.kfor1240.com/pages/15150062.php</td>\n",
       "      <td>kfor1240.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21733231</th>\n",
       "      <td>http://www.kvia.com/news/man-crossing-street-s...</td>\n",
       "      <td>kvia.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38129972</th>\n",
       "      <td>http://www.harpersbazaar.com/culture/film-tv/i...</td>\n",
       "      <td>harpersbazaar.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4006794</th>\n",
       "      <td>https://www.stgeorgeutah.com/news/archive/2015...</td>\n",
       "      <td>stgeorgeutah.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19866719</th>\n",
       "      <td>http://www.ktvu.com/news/congresswoman-speier-...</td>\n",
       "      <td>ktvu.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24707185</th>\n",
       "      <td>http://ftalphaville.ft.com/blog/2008/10/15/170...</td>\n",
       "      <td>ft.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42018735</th>\n",
       "      <td>https://www.foxnews.com/us/jury-selection-to-b...</td>\n",
       "      <td>foxnews.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18092596</th>\n",
       "      <td>https://www.healthcentral.com/article/4-risk-f...</td>\n",
       "      <td>healthcentral.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9876384</th>\n",
       "      <td>http://activate.metroactive.com/2014/02/morris...</td>\n",
       "      <td>metroactive.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        url  \\\n",
       "21147334  http://www.hindustantimes.com/india/wife-of-ma...   \n",
       "15195719         http://www.kfor1240.com/pages/15150062.php   \n",
       "21733231  http://www.kvia.com/news/man-crossing-street-s...   \n",
       "38129972  http://www.harpersbazaar.com/culture/film-tv/i...   \n",
       "4006794   https://www.stgeorgeutah.com/news/archive/2015...   \n",
       "19866719  http://www.ktvu.com/news/congresswoman-speier-...   \n",
       "24707185  http://ftalphaville.ft.com/blog/2008/10/15/170...   \n",
       "42018735  https://www.foxnews.com/us/jury-selection-to-b...   \n",
       "18092596  https://www.healthcentral.com/article/4-risk-f...   \n",
       "9876384   http://activate.metroactive.com/2014/02/morris...   \n",
       "\n",
       "                      domain  \n",
       "21147334  hindustantimes.com  \n",
       "15195719        kfor1240.com  \n",
       "21733231            kvia.com  \n",
       "38129972   harpersbazaar.com  \n",
       "4006794     stgeorgeutah.com  \n",
       "19866719            ktvu.com  \n",
       "24707185              ft.com  \n",
       "42018735         foxnews.com  \n",
       "18092596   healthcentral.com  \n",
       "9876384      metroactive.com  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2212fdae-9deb-4901-9474-8d12cd9685c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_labels['is_news'] = ground_truth_labels['label'].apply(lambda x: 1 if 'News' in x else 0)\n",
    "domain_news_map = ground_truth_labels[['domain','is_news']].copy()\n",
    "\n",
    "filtered_df = filtered_df.merge(domain_news_map, on='domain', how='left')\n",
    "\n",
    "# Prepare training and testing data\n",
    "train_domains = ground_truth_labels[ground_truth_labels['set'].str.contains('train')]['domain']\n",
    "test_domains = ground_truth_labels[ground_truth_labels['set'].str.contains('test')]['domain']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ceea2a7-c417-4e9a-b198-35cb0343e00f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_news\n",
       "1    38574202\n",
       "0     3615578\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df.is_news.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54443f94-0916-43af-868e-e0254d98f517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample for testing\n",
    "filtered_df = filtered_df.sample(100000)\n",
    "\n",
    "# Separate training and testing data\n",
    "X_train = filtered_df[filtered_df['domain'].isin(train_domains)]\n",
    "X_test = filtered_df[filtered_df['domain'].isin(test_domains)]\n",
    "\n",
    "# Prepare labels\n",
    "y_train = X_train['is_news']\n",
    "y_test = X_test['is_news']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26959a81-bf52-4acf-b084-d930187013ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fdac53-710d-4325-9574-86755198556c",
   "metadata": {},
   "source": [
    "# Helper Functions for the Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e8997a9-74f5-4302-a14b-18bb09262fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vectorization_strategies():\n",
    "    \"\"\"\n",
    "    Define different vectorization strategies\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of vectorization strategies\n",
    "    \"\"\"\n",
    "    return {\n",
    "        # Domain Vectorization Strategies\n",
    "        'Domain (Char 2-5 grams)': TfidfVectorizer(\n",
    "            analyzer='char', \n",
    "            ngram_range=(2, 5), \n",
    "            max_features=500\n",
    "        ),\n",
    "        'Domain (Word 1-5 grams)': TfidfVectorizer(\n",
    "            analyzer='word', \n",
    "            ngram_range=(1, 5), \n",
    "            max_features=500,\n",
    "            token_pattern=r'[a-zA-Z0-9]+'\n",
    "        ),\n",
    "        \n",
    "        # Path Vectorization Strategies\n",
    "        'Path (Char 2-5 grams)': TfidfVectorizer(\n",
    "            analyzer='char', \n",
    "            ngram_range=(2, 5), \n",
    "            max_features=500\n",
    "        ),\n",
    "        'Path (Word 1-5 grams)': TfidfVectorizer(\n",
    "            analyzer='word', \n",
    "            ngram_range=(1, 5), \n",
    "            max_features=500,\n",
    "            token_pattern=r'[a-zA-Z0-9]+'\n",
    "        ),\n",
    "        \n",
    "        # Combined Feature Vectorizers\n",
    "        'Combined (Domain Char + Path Word)': None  # Will be handled separately\n",
    "    }\n",
    "\n",
    "def prepare_features(train_df, test_df, vectorization_strategies):\n",
    "    \"\"\"\n",
    "    Prepare features using different vectorization strategies\n",
    "    \n",
    "    Args:\n",
    "        train_df: Training dataframe\n",
    "        test_df: Testing dataframe\n",
    "        vectorization_strategies: Dictionary of vectorization strategies\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of feature matrices\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # Individual Vectorization Strategies\n",
    "    for strategy_name, vectorizer in vectorization_strategies.items():\n",
    "        if strategy_name == 'Domain (Char 2-5 grams)':\n",
    "            features[strategy_name] = {\n",
    "                'X_train': vectorizer.fit_transform(train_df['domain']),\n",
    "                'X_test': vectorizer.transform(test_df['domain'])\n",
    "            }\n",
    "        elif strategy_name == 'Domain (Word 1-5 grams)':\n",
    "            features[strategy_name] = {\n",
    "                'X_train': vectorizer.fit_transform(train_df['domain']),\n",
    "                'X_test': vectorizer.transform(test_df['domain'])\n",
    "            }\n",
    "        elif strategy_name == 'Path (Char 2-5 grams)':\n",
    "            features[strategy_name] = {\n",
    "                'X_train': vectorizer.fit_transform(train_df['url']),\n",
    "                'X_test': vectorizer.transform(test_df['url'])\n",
    "            }\n",
    "        elif strategy_name == 'Path (Word 1-5 grams)':\n",
    "            features[strategy_name] = {\n",
    "                'X_train': vectorizer.fit_transform(train_df['url']),\n",
    "                'X_test': vectorizer.transform(test_df['url'])\n",
    "            }\n",
    "    \n",
    "    # Combined Feature Strategy\n",
    "    domain_char_vec = TfidfVectorizer(analyzer='char', ngram_range=(2, 5), max_features=500)\n",
    "    path_word_vec = TfidfVectorizer(analyzer='word', ngram_range=(1, 5), max_features=500, token_pattern=r'[a-zA-Z0-9]+')\n",
    "    \n",
    "    features['Combined (Domain Char + Path Word)'] = {\n",
    "        'X_train': hstack([\n",
    "            domain_char_vec.fit_transform(train_df['domain']),\n",
    "            path_word_vec.fit_transform(train_df['url'])\n",
    "        ]),\n",
    "        'X_test': hstack([\n",
    "            domain_char_vec.transform(test_df['domain']),\n",
    "            path_word_vec.transform(test_df['url'])\n",
    "        ])\n",
    "    }\n",
    "    \n",
    "    return features\n",
    "\n",
    "def create_evaluation_pipeline(model):\n",
    "    \"\"\"\n",
    "    Create a scikit-learn pipeline for model evaluation\n",
    "    \n",
    "    Args:\n",
    "        model: Sklearn model to use\n",
    "    \n",
    "    Returns:\n",
    "        sklearn Pipeline\n",
    "    \"\"\"\n",
    "    return Pipeline([\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "\n",
    "def evaluate_model(name, X_train, X_test, y_train, y_test, model_factory=LogisticRegression):\n",
    "    \"\"\"\n",
    "    Evaluate a machine learning model\n",
    "    \n",
    "    Args:\n",
    "        name: Name of the model\n",
    "        X_train, X_test: Training and testing features\n",
    "        y_train, y_test: Training and testing labels\n",
    "        model_factory: Function to create model instance\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with model performance metrics\n",
    "    \"\"\"\n",
    "    # Create model\n",
    "    model = create_evaluation_pipeline(model_factory(random_state=42))\n",
    "    \n",
    "    # Training\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # Prediction\n",
    "    start_time = time.time()\n",
    "    y_pred = model.predict(X_test)\n",
    "    inference_time = time.time() - start_time\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    f2 = fbeta_score(y_test, y_pred, beta=2, average='weighted')\n",
    "    \n",
    "    print(f\"\\n{name} Results:\")\n",
    "    print(f\"Training time: {train_time:.4f} seconds\")\n",
    "    print(f\"Inference time: {inference_time:.4f} seconds\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"F2 Score: {f2:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    return {\n",
    "        'Vectorization Strategy': name,\n",
    "        'Train Time (s)': train_time,\n",
    "        'Inference Time (s)': inference_time,\n",
    "        'Accuracy': accuracy,\n",
    "        'F1 Score': f1,\n",
    "        'F2 Score': f2\n",
    "    }\n",
    "\n",
    "def compare_vectorization_strategies(train_df, test_df, y_train, y_test, \n",
    "                                     models=None, \n",
    "                                     vectorization_strategies=None):\n",
    "    \"\"\"\n",
    "    Compare different vectorization strategies and models\n",
    "    \n",
    "    Args:\n",
    "        train_df: Training dataframe\n",
    "        test_df: Testing dataframe\n",
    "        y_train, y_test: Training and testing labels\n",
    "        models: List of models to evaluate (default: Logistic Regression)\n",
    "        vectorization_strategies: Dictionary of vectorization strategies\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with performance metrics\n",
    "    \"\"\"\n",
    "    # Default models if not provided\n",
    "    if models is None:\n",
    "        models = [\n",
    "            ('Logistic Regression', LogisticRegression),\n",
    "            ('Random Forest', RandomForestClassifier),\n",
    "            ('SVM', SVC)\n",
    "        ]\n",
    "    \n",
    "    # Default vectorization strategies if not provided\n",
    "    if vectorization_strategies is None:\n",
    "        vectorization_strategies = create_vectorization_strategies()\n",
    "    \n",
    "    # Prepare features\n",
    "    feature_sets = prepare_features(train_df, test_df, vectorization_strategies)\n",
    "    \n",
    "    # Results storage\n",
    "    all_results = []\n",
    "    \n",
    "    # Evaluate each vectorization strategy with each model\n",
    "    for strategy_name, feature_set in feature_sets.items():\n",
    "        for model_name, model_factory in models:\n",
    "            full_name = f\"{strategy_name} - {model_name}\"\n",
    "            \n",
    "            # Evaluate model\n",
    "            result = evaluate_model(\n",
    "                full_name, \n",
    "                feature_set['X_train'], \n",
    "                feature_set['X_test'], \n",
    "                y_train, \n",
    "                y_test, \n",
    "                model_factory\n",
    "            )\n",
    "            all_results.append(result)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    results_df = results_df.sort_values('Accuracy', ascending=False)\n",
    "    \n",
    "    print(\"\\nVectorization and Model Comparison Summary:\")\n",
    "    print(results_df)\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ba280b-a321-47e9-b42f-611d8ccdeb10",
   "metadata": {},
   "source": [
    "# Actual Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c47c0d-8c01-42dd-a94e-4a31b5a243f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Domain (Char 2-5 grams) - Logistic Regression Results:\n",
      "Training time: 0.4472 seconds\n",
      "Inference time: 0.0004 seconds\n",
      "Accuracy: 0.9181\n",
      "F1 Score: 0.9016\n",
      "F2 Score: 0.9113\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.14      0.05      0.08       586\n",
      "           1       0.94      0.98      0.96      8686\n",
      "\n",
      "    accuracy                           0.92      9272\n",
      "   macro avg       0.54      0.52      0.52      9272\n",
      "weighted avg       0.89      0.92      0.90      9272\n",
      "\n",
      "\n",
      "Domain (Char 2-5 grams) - Random Forest Results:\n",
      "Training time: 19.2749 seconds\n",
      "Inference time: 0.1363 seconds\n",
      "Accuracy: 0.9363\n",
      "F1 Score: 0.9280\n",
      "F2 Score: 0.9326\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.28      0.36       586\n",
      "           1       0.95      0.98      0.97      8686\n",
      "\n",
      "    accuracy                           0.94      9272\n",
      "   macro avg       0.72      0.63      0.66      9272\n",
      "weighted avg       0.92      0.94      0.93      9272\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = compare_vectorization_strategies(\n",
    "    X_train, \n",
    "    X_test, \n",
    "    y_train, \n",
    "    y_test,\n",
    "    \n",
    "    # Optional: customize models\n",
    "    models=[\n",
    "        ('Logistic Regression', LogisticRegression),\n",
    "        ('Random Forest', RandomForestClassifier),\n",
    "        ('SVM', SVC)\n",
    "    ],\n",
    "    \n",
    "    # Optional: customize vectorization strategies\n",
    "    vectorization_strategies=create_vectorization_strategies()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f97d7cc-c1f4-4452-b2f3-9f9cc6367ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9e2e12-89b7-4bed-9384-43af397e1b70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
