{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "import duckdb\n",
    "from openai import OpenAI\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"SELECT url FROM 'hf://datasets/nhagar/falcon_urls/data/train-00024-of-00170.parquet' LIMIT 100\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp = con.execute(q).fetchdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp[\"is_news\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp.to_csv(\"../data/falcon_urls_sample.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp_labeled = con.execute(\"SELECT * FROM '../data/falcon_urls_sample.csv'\").fetchdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('prompt.txt', 'r') as file:\n",
    "    prompt = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(base_url=\"http://127.0.0.1:1234/v1\", api_key=\"lm-studio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_url(url, model):\n",
    "    resp = llm.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": prompt},\n",
    "            {\"role\": \"user\", \"content\": url},\n",
    "        ],\n",
    "    )\n",
    "    txt = resp.choices[0].message.content\n",
    "\n",
    "    json_extract_pattern = re.compile(r\"```json\\n(.*?)\\n```\", re.DOTALL)\n",
    "    json_extract = json_extract_pattern.search(txt).group(1)\n",
    "\n",
    "    return json.loads(json_extract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    #\"llama-3.2-3b-instruct-4bit\",\n",
    "    #\"qwen2.5-7b-instruct-1m\",\n",
    "    \"gemma-2-9b-it-GGUF\",\n",
    "    #\"qwen2.5-14b-instruct-1m\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for model in models:\n",
    "    samp_labeled[f\"{model}_label\"] = samp_labeled[\"url\"].apply(classify_url, model=model)\n",
    "    samp_labeled[f\"{model}_is_news\"] = samp_labeled[f\"{model}_label\"].apply(lambda x: x[\"is_news\"])\n",
    "    samp_labeled[f\"{model}_reason\"] = samp_labeled[f\"{model}_label\"].apply(lambda x: x[\"reason\"])\n",
    "    results[model] = {\n",
    "        \"accuracy\": accuracy_score(samp_labeled[\"is_news\"], samp_labeled[f\"{model}_is_news\"]),\n",
    "        \"precision\": precision_score(samp_labeled[\"is_news\"], samp_labeled[f\"{model}_is_news\"]),\n",
    "        \"recall\": recall_score(samp_labeled[\"is_news\"], samp_labeled[f\"{model}_is_news\"]),\n",
    "        \"f1\": f1_score(samp_labeled[\"is_news\"], samp_labeled[f\"{model}_is_news\"]),\n",
    "    }\n",
    "\n",
    "    with open(f\"../data/model_results_{model}.json\", \"w\") as f:\n",
    "        json.dump(results, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp_labeled.to_csv('../data/sample_gemma_labeled.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "# Start with all domains from Common Crawl as input\n",
    "\n",
    "# Two maps needed at start\n",
    "# Domain → datasets mapping: Associate each domain with a set of datasets that use it\n",
    "# Domain → URL sample mapping: For each domain, select 3 URL samples\n",
    "\n",
    "#For each sampled URL:\n",
    "    # Classify each URL using the ModernBERT classifier\n",
    "    # classifier analyzes slug to determine if it represents news content\n",
    "    # Labels sampled URLs\n",
    "# Domains now have news_domain label (True if 4/7 or more URLs were labeled is_news)\n",
    "\n",
    "# grab raw datasets for C4 and other large datasets (don't use hf library)\n",
    "# build off dataset pipeline logic (loop of batches)\n",
    "# need a deduplicated index, 3 URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
