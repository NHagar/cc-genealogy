{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ab61a3e-d66d-485a-9264-1fd4542be339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load \n",
    "import dask.dataframe as dd\n",
    "\n",
    "# total urls: 365,233,500\n",
    "df = dd.read_parquet(\"hf://datasets/nhagar/c4_en_urls/data/train-*.parquet\")\n",
    "df100 = df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfc59ead-7d0a-4bb4-8158-e55c1cafee74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Domains</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4037</th>\n",
       "      <td>www.sustaincase.com</td>\n",
       "      <td>News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1334</th>\n",
       "      <td>www.epdf.tips</td>\n",
       "      <td>Science, Academia, &amp; Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>www.jotform.com</td>\n",
       "      <td>Business &amp; E-Commerce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>www.marketplace.org</td>\n",
       "      <td>News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4094</th>\n",
       "      <td>www.biogeoamb.grupos.uniovi.es</td>\n",
       "      <td>General Information &amp; Education</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 URL                          Domains\n",
       "4037             www.sustaincase.com                             News\n",
       "1334                   www.epdf.tips  Science, Academia, & Technology\n",
       "41                   www.jotform.com            Business & E-Commerce\n",
       "512              www.marketplace.org                             News\n",
       "4094  www.biogeoamb.grupos.uniovi.es  General Information & Education"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "labeled_domains = pd.read_csv('../data/url_domain_1to1_mappings.csv')\n",
    "labeled_domains.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0737b115-6ced-4036-8d50-2a38fcce346c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "import re\n",
    "\n",
    "with open('prompt_is_news.txt', 'r') as file:\n",
    "    prompt_binary_label = file.read()\n",
    "    \n",
    "with open('prompt_service_label.txt', 'r') as file:\n",
    "    prompt_service_label = file.read()\n",
    "    \n",
    "llm = OpenAI(base_url=\"http://127.0.0.1:1234/v1\", api_key=\"lm-studio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c25f8053-43df-40db-b6c6-7bb091b6d90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_url(url, model, prompt):\n",
    "    resp = llm.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": prompt},\n",
    "            {\"role\": \"user\", \"content\": url},\n",
    "        ],\n",
    "    )\n",
    "    txt = resp.choices[0].message.content\n",
    "\n",
    "    json_extract_pattern = re.compile(r\"```json\\n(.*?)\\n```\", re.DOTALL)\n",
    "    json_extract = json_extract_pattern.search(txt).group(1)\n",
    "\n",
    "    return json.loads(json_extract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b4c4154-ebc6-4a48-be6e-67883d256576",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_1M = df.sample(frac=0.0001).compute().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccc37ea-79f6-4b76-922b-258a3cfec484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Processing dataframe: sample_10 =====\n",
      "\n",
      "Processing model: gemma-2-9b-it-GGUF on dataframe: sample_10\n",
      "Starting binary classification for gemma-2-9b-it-GGUF...\n",
      "  Completed binary classification in 51.11s (5.1111s per URL)\n",
      "Starting service classification for gemma-2-9b-it-GGUF...\n",
      "  Completed service classification in 54.27s (5.4271s per URL)\n",
      "Model gemma-2-9b-it-GGUF completed on sample_10\n",
      "  Total time: 105.38s (10.5382s per URL)\n",
      "Saved results for sample_10 to ../data/sample_10_all_models.csv\n",
      "\n",
      "===== Processing dataframe: sample_100 =====\n",
      "\n",
      "Processing model: gemma-2-9b-it-GGUF on dataframe: sample_100\n",
      "Starting binary classification for gemma-2-9b-it-GGUF...\n",
      "  Completed binary classification in 529.32s (5.2932s per URL)\n",
      "Starting service classification for gemma-2-9b-it-GGUF...\n",
      "  Completed service classification in 518.91s (5.1891s per URL)\n",
      "Model gemma-2-9b-it-GGUF completed on sample_100\n",
      "  Total time: 1048.23s (10.4823s per URL)\n",
      "Saved results for sample_100 to ../data/sample_100_all_models.csv\n",
      "\n",
      "===== Processing dataframe: sample_1M =====\n",
      "\n",
      "Processing model: gemma-2-9b-it-GGUF on dataframe: sample_1M\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# List of models to evaluate\n",
    "models = [\n",
    "    #\"llama-3.2-3b-instruct-4bit\",\n",
    "    #\"qwen2.5-7b-instruct-1m\",\n",
    "    \"gemma-2-9b-it-GGUF\",\n",
    "    #\"qwen2.5-14b-instruct-1m\"\n",
    "]\n",
    "\n",
    "dataframes = {\n",
    "    \"sample_10\": df100.sample(10).copy(),\n",
    "    \"sample_100\": df100.copy(),\n",
    "    \"sample_1M\": df.sample(frac=0.001).copy(),\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "# Outer loop for each dataframe\n",
    "for df_name, df in dataframes.items():\n",
    "    print(f\"\\n===== Processing dataframe: {df_name} =====\")\n",
    "    results[df_name] = {}\n",
    "\n",
    "    df[\"is_news\"] = 0\n",
    "    df[\"label\"] = \"\"\n",
    "\n",
    "    for model in models:\n",
    "        print(f\"\\nProcessing model: {model} on dataframe: {df_name}\")\n",
    "\n",
    "        results[df_name][model] = {\n",
    "            \"binary_classification_time\": 0,\n",
    "            \"service_classification_time\": 0,\n",
    "            \"total_time\": 0,\n",
    "            \"start_time\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"url_count\": len(df)\n",
    "        }\n",
    "        \n",
    "        # Time the binary classification\n",
    "        print(f\"Starting binary classification for {model}...\")\n",
    "        start_time = time.time()\n",
    "        df[f\"{model}_binary_label\"] = df[\"url\"].apply(\n",
    "            classify_url, model=model, prompt=prompt_binary_label\n",
    "        )\n",
    "        binary_time = time.time() - start_time\n",
    "        results[df_name][model][\"binary_classification_time\"] = binary_time\n",
    "        print(f\"  Completed binary classification in {binary_time:.2f}s ({binary_time/len(df):.4f}s per URL)\")\n",
    "        \n",
    "        # Time the service classification\n",
    "        print(f\"Starting service classification for {model}...\")\n",
    "        start_time = time.time()\n",
    "        df[f\"{model}_service_label\"] = df[\"url\"].apply(\n",
    "            classify_url, model=model, prompt=prompt_service_label\n",
    "        )\n",
    "        service_time = time.time() - start_time\n",
    "        results[df_name][model][\"service_classification_time\"] = service_time\n",
    "        print(f\"  Completed service classification in {service_time:.2f}s ({service_time/len(df):.4f}s per URL)\")\n",
    "        \n",
    "        # Calculate derived fields\n",
    "        df[f\"{model}_is_news\"] = df[f\"{model}_binary_label\"].apply(\n",
    "            lambda x: x[\"is_news\"]\n",
    "        )\n",
    "        df[f\"{model}_label\"] = df[f\"{model}_service_label\"].apply(\n",
    "            lambda x: x[\"label\"]\n",
    "        )\n",
    "        \n",
    "        # Record total time and end time\n",
    "        total_time = binary_time + service_time\n",
    "        results[df_name][model][\"total_time\"] = total_time\n",
    "        results[df_name][model][\"end_time\"] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        results[df_name][model][\"avg_time_per_url\"] = total_time / len(df)\n",
    "        \n",
    "        print(f\"Model {model} completed on {df_name}\")\n",
    "        print(f\"  Total time: {total_time:.2f}s ({total_time/len(df):.4f}s per URL)\")\n",
    "        \n",
    "        # Save intermediate results after each model\n",
    "        with open(f\"../data/results_{df_name}_{model}.json\", \"w\") as f:\n",
    "            json.dump(results[df_name][model], f, indent=2)\n",
    "    \n",
    "    # Save the dataframe with all results for this dataset\n",
    "    output_filename = f\"../data/{df_name}_all_models.csv\"\n",
    "    df.to_csv(output_filename, index=False)\n",
    "    print(f\"Saved results for {df_name} to {output_filename}\")\n",
    "\n",
    "# Save final consolidated results\n",
    "with open(\"../data/all_experiments_results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"\\nAll experiments completed successfully!\")\n",
    "print(f\"Final results saved to ../data/all_experiments_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2541fcb-f56e-4e6e-b58a-9affeb3366cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "    results[model] = {\n",
    "        \"accuracy\": accuracy_score(samp_labeled[\"is_news\"], samp_labeled[f\"{model}_is_news\"]),\n",
    "        \"precision\": precision_score(samp_labeled[\"is_news\"], samp_labeled[f\"{model}_is_news\"]),\n",
    "        \"recall\": recall_score(samp_labeled[\"is_news\"], samp_labeled[f\"{model}_is_news\"]),\n",
    "        \"f1\": f1_score(samp_labeled[\"is_news\"], samp_labeled[f\"{model}_is_news\"]),\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
